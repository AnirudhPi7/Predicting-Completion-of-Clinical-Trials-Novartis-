{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from pytorch_tabular.models.tab_transformer import TabTransformerModel\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 257577 entries, 0 to 257576\n",
      "Data columns (total 22 columns):\n",
      " #   Column                      Non-Null Count   Dtype  \n",
      "---  ------                      --------------   -----  \n",
      " 0   Unnamed: 0                  257577 non-null  int64  \n",
      " 1   NCT Number                  257577 non-null  object \n",
      " 2   Study Title                 257577 non-null  object \n",
      " 3   Study Status                257577 non-null  object \n",
      " 4   Brief Summary               257577 non-null  object \n",
      " 5   Conditions                  257577 non-null  object \n",
      " 6   Primary Outcome Measures    247086 non-null  object \n",
      " 7   Secondary Outcome Measures  185779 non-null  object \n",
      " 8   Other Outcome Measures      18272 non-null   object \n",
      " 9   Sponsor                     257577 non-null  object \n",
      " 10  Collaborators               83679 non-null   object \n",
      " 11  Sex                         257317 non-null  object \n",
      " 12  Age                         257577 non-null  object \n",
      " 13  Phases                      112765 non-null  object \n",
      " 14  Enrollment                  254205 non-null  float64\n",
      " 15  Funder Type                 257577 non-null  object \n",
      " 16  Study Type                  257577 non-null  object \n",
      " 17  Study Design                257577 non-null  object \n",
      " 18  Start Month                 257577 non-null  int64  \n",
      " 19  Start Quarter               257577 non-null  int64  \n",
      " 20  Condition Category          257577 non-null  object \n",
      " 21  Conditions_Category         257577 non-null  object \n",
      "dtypes: float64(1), int64(3), object(18)\n",
      "memory usage: 43.2+ MB\n",
      "Initial Dataset Info:\n",
      " None\n",
      "\n",
      "Sample Data:\n",
      "    Unnamed: 0   NCT Number                                        Study Title  \\\n",
      "0           0  NCT00559130  Efficacy Study of CytoSorb Hemoperfusion Devic...   \n",
      "1           1  NCT00937664  Safety and Tolerability Study of AZD7762 in Co...   \n",
      "2           2  NCT00441597  Does Atorvastatin Reduce Ischemia-Reperfusion ...   \n",
      "3           3  NCT03296228  Comparison of Dynamic Radiographs in Determini...   \n",
      "4           4  NCT00421603  A Placebo-Controlled Study of Mixed Amphetamin...   \n",
      "\n",
      "    Study Status                                      Brief Summary  \\\n",
      "0      Completed  The hypothesis of this study is use of CytoSor...   \n",
      "1  Not_Completed  The primary purpose of this study is to find o...   \n",
      "2      Completed  To study the impact of 3 day exposure to atorv...   \n",
      "3      Completed  The purpose of this study is to identify the f...   \n",
      "4      Completed  The proposed protocol is a double-blind, place...   \n",
      "\n",
      "                                          Conditions  \\\n",
      "0  Acute Respiratory Distress Syndrome|Acute Lung...   \n",
      "1    Cancer|Solid Tumors|Advanced Solid Malignancies   \n",
      "2  Ischemia Reperfusion Injury|Cardiovascular Dis...   \n",
      "3                    Adolescent Idiopathic Scoliosis   \n",
      "4                                 Cocaine Dependence   \n",
      "\n",
      "                            Primary Outcome Measures  \\\n",
      "0  Relative IL-6 levels as a percent (%) of basel...   \n",
      "1  Assessment of adverse events (based on CTCAE v...   \n",
      "2  Annexin A 5 targeting in the non dominant then...   \n",
      "3  Investigate the flexibility equivalence of dif...   \n",
      "4  Three Weeks of Continuous Cocaine Abstinence a...   \n",
      "\n",
      "                          Secondary Outcome Measures Other Outcome Measures  \\\n",
      "0  Ventilator Free Days, Reduction cytokines TNF-...                    NaN   \n",
      "1  Pharmacokinetic effect of AZD7762 when adminis...                    NaN   \n",
      "2  workload during ischemic exercise, workload du...                    NaN   \n",
      "3  Incorporate these findings into the Lenke Clas...                    NaN   \n",
      "4                                                NaN                    NaN   \n",
      "\n",
      "                                Sponsor  ...                 Age  Phases  \\\n",
      "0            MedaSorb Technologies, Inc  ...  ADULT, OLDER_ADULT     NaN   \n",
      "1                           AstraZeneca  ...  ADULT, OLDER_ADULT  PHASE1   \n",
      "2     Radboud University Medical Center  ...               ADULT  PHASE4   \n",
      "3           The University of Hong Kong  ...        CHILD, ADULT     NaN   \n",
      "4  New York State Psychiatric Institute  ...               ADULT  PHASE2   \n",
      "\n",
      "  Enrollment Funder Type      Study Type  \\\n",
      "0      100.0    INDUSTRY  INTERVENTIONAL   \n",
      "1       24.0    INDUSTRY  INTERVENTIONAL   \n",
      "2       30.0       OTHER  INTERVENTIONAL   \n",
      "3      134.0       OTHER   OBSERVATIONAL   \n",
      "4       81.0       OTHER  INTERVENTIONAL   \n",
      "\n",
      "                                        Study Design Start Month  \\\n",
      "0  Allocation: RANDOMIZED|Intervention Model: PAR...          11   \n",
      "1  Allocation: NON_RANDOMIZED|Intervention Model:...           7   \n",
      "2  Allocation: RANDOMIZED|Intervention Model: CRO...           2   \n",
      "3          Observational Model: |Time Perspective: p          -1   \n",
      "4  Allocation: RANDOMIZED|Intervention Model: PAR...           2   \n",
      "\n",
      "  Start Quarter          Condition Category         Conditions_Category  \n",
      "0             4  Other Rare or Unclassified  Other Rare or Unclassified  \n",
      "1             3                    Oncology                    Oncology  \n",
      "2             1  Other Rare or Unclassified  Other Rare or Unclassified  \n",
      "3            -1  Other Rare or Unclassified                Non-Oncology  \n",
      "4             1  Other Rare or Unclassified                Non-Oncology  \n",
      "\n",
      "[5 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load Dataset\n",
    "def load_data(file_path):\n",
    "    \"\"\"Loads the dataset and provides an initial overview.\"\"\"\n",
    "    data = pd.read_csv(file_path)\n",
    "    print(\"Initial Dataset Info:\\n\", data.info())\n",
    "    print(\"\\nSample Data:\\n\", data.head())\n",
    "    return data\n",
    "data = load_data(\"C:\\\\Users\\\\sriha\\\\Music\\\\Case Comps\\\\NEST\\\\Data\\\\category_updated.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>NCT Number</th>\n",
       "      <th>Study Title</th>\n",
       "      <th>Study Status</th>\n",
       "      <th>Brief Summary</th>\n",
       "      <th>Conditions</th>\n",
       "      <th>Primary Outcome Measures</th>\n",
       "      <th>Secondary Outcome Measures</th>\n",
       "      <th>Other Outcome Measures</th>\n",
       "      <th>Sponsor</th>\n",
       "      <th>...</th>\n",
       "      <th>Age</th>\n",
       "      <th>Phases</th>\n",
       "      <th>Enrollment</th>\n",
       "      <th>Funder Type</th>\n",
       "      <th>Study Type</th>\n",
       "      <th>Study Design</th>\n",
       "      <th>Start Month</th>\n",
       "      <th>Start Quarter</th>\n",
       "      <th>Condition Category</th>\n",
       "      <th>Conditions_Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NCT00559130</td>\n",
       "      <td>Efficacy Study of CytoSorb Hemoperfusion Devic...</td>\n",
       "      <td>Completed</td>\n",
       "      <td>The hypothesis of this study is use of CytoSor...</td>\n",
       "      <td>Acute Respiratory Distress Syndrome|Acute Lung...</td>\n",
       "      <td>Relative IL-6 levels as a percent (%) of basel...</td>\n",
       "      <td>Ventilator Free Days, Reduction cytokines TNF-...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MedaSorb Technologies, Inc</td>\n",
       "      <td>...</td>\n",
       "      <td>ADULT, OLDER_ADULT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>INDUSTRY</td>\n",
       "      <td>INTERVENTIONAL</td>\n",
       "      <td>Allocation: RANDOMIZED|Intervention Model: PAR...</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>Other Rare or Unclassified</td>\n",
       "      <td>Other Rare or Unclassified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>NCT00937664</td>\n",
       "      <td>Safety and Tolerability Study of AZD7762 in Co...</td>\n",
       "      <td>Not_Completed</td>\n",
       "      <td>The primary purpose of this study is to find o...</td>\n",
       "      <td>Cancer|Solid Tumors|Advanced Solid Malignancies</td>\n",
       "      <td>Assessment of adverse events (based on CTCAE v...</td>\n",
       "      <td>Pharmacokinetic effect of AZD7762 when adminis...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AstraZeneca</td>\n",
       "      <td>...</td>\n",
       "      <td>ADULT, OLDER_ADULT</td>\n",
       "      <td>PHASE1</td>\n",
       "      <td>24.0</td>\n",
       "      <td>INDUSTRY</td>\n",
       "      <td>INTERVENTIONAL</td>\n",
       "      <td>Allocation: NON_RANDOMIZED|Intervention Model:...</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>Oncology</td>\n",
       "      <td>Oncology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>NCT00441597</td>\n",
       "      <td>Does Atorvastatin Reduce Ischemia-Reperfusion ...</td>\n",
       "      <td>Completed</td>\n",
       "      <td>To study the impact of 3 day exposure to atorv...</td>\n",
       "      <td>Ischemia Reperfusion Injury|Cardiovascular Dis...</td>\n",
       "      <td>Annexin A 5 targeting in the non dominant then...</td>\n",
       "      <td>workload during ischemic exercise, workload du...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Radboud University Medical Center</td>\n",
       "      <td>...</td>\n",
       "      <td>ADULT</td>\n",
       "      <td>PHASE4</td>\n",
       "      <td>30.0</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>INTERVENTIONAL</td>\n",
       "      <td>Allocation: RANDOMIZED|Intervention Model: CRO...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Other Rare or Unclassified</td>\n",
       "      <td>Other Rare or Unclassified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>NCT03296228</td>\n",
       "      <td>Comparison of Dynamic Radiographs in Determini...</td>\n",
       "      <td>Completed</td>\n",
       "      <td>The purpose of this study is to identify the f...</td>\n",
       "      <td>Adolescent Idiopathic Scoliosis</td>\n",
       "      <td>Investigate the flexibility equivalence of dif...</td>\n",
       "      <td>Incorporate these findings into the Lenke Clas...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The University of Hong Kong</td>\n",
       "      <td>...</td>\n",
       "      <td>CHILD, ADULT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>134.0</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>OBSERVATIONAL</td>\n",
       "      <td>Observational Model: |Time Perspective: p</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>Other Rare or Unclassified</td>\n",
       "      <td>Non-Oncology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>NCT00421603</td>\n",
       "      <td>A Placebo-Controlled Study of Mixed Amphetamin...</td>\n",
       "      <td>Completed</td>\n",
       "      <td>The proposed protocol is a double-blind, place...</td>\n",
       "      <td>Cocaine Dependence</td>\n",
       "      <td>Three Weeks of Continuous Cocaine Abstinence a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>New York State Psychiatric Institute</td>\n",
       "      <td>...</td>\n",
       "      <td>ADULT</td>\n",
       "      <td>PHASE2</td>\n",
       "      <td>81.0</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>INTERVENTIONAL</td>\n",
       "      <td>Allocation: RANDOMIZED|Intervention Model: PAR...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Other Rare or Unclassified</td>\n",
       "      <td>Non-Oncology</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0   NCT Number                                        Study Title  \\\n",
       "0           0  NCT00559130  Efficacy Study of CytoSorb Hemoperfusion Devic...   \n",
       "1           1  NCT00937664  Safety and Tolerability Study of AZD7762 in Co...   \n",
       "2           2  NCT00441597  Does Atorvastatin Reduce Ischemia-Reperfusion ...   \n",
       "3           3  NCT03296228  Comparison of Dynamic Radiographs in Determini...   \n",
       "4           4  NCT00421603  A Placebo-Controlled Study of Mixed Amphetamin...   \n",
       "\n",
       "    Study Status                                      Brief Summary  \\\n",
       "0      Completed  The hypothesis of this study is use of CytoSor...   \n",
       "1  Not_Completed  The primary purpose of this study is to find o...   \n",
       "2      Completed  To study the impact of 3 day exposure to atorv...   \n",
       "3      Completed  The purpose of this study is to identify the f...   \n",
       "4      Completed  The proposed protocol is a double-blind, place...   \n",
       "\n",
       "                                          Conditions  \\\n",
       "0  Acute Respiratory Distress Syndrome|Acute Lung...   \n",
       "1    Cancer|Solid Tumors|Advanced Solid Malignancies   \n",
       "2  Ischemia Reperfusion Injury|Cardiovascular Dis...   \n",
       "3                    Adolescent Idiopathic Scoliosis   \n",
       "4                                 Cocaine Dependence   \n",
       "\n",
       "                            Primary Outcome Measures  \\\n",
       "0  Relative IL-6 levels as a percent (%) of basel...   \n",
       "1  Assessment of adverse events (based on CTCAE v...   \n",
       "2  Annexin A 5 targeting in the non dominant then...   \n",
       "3  Investigate the flexibility equivalence of dif...   \n",
       "4  Three Weeks of Continuous Cocaine Abstinence a...   \n",
       "\n",
       "                          Secondary Outcome Measures Other Outcome Measures  \\\n",
       "0  Ventilator Free Days, Reduction cytokines TNF-...                    NaN   \n",
       "1  Pharmacokinetic effect of AZD7762 when adminis...                    NaN   \n",
       "2  workload during ischemic exercise, workload du...                    NaN   \n",
       "3  Incorporate these findings into the Lenke Clas...                    NaN   \n",
       "4                                                NaN                    NaN   \n",
       "\n",
       "                                Sponsor  ...                 Age  Phases  \\\n",
       "0            MedaSorb Technologies, Inc  ...  ADULT, OLDER_ADULT     NaN   \n",
       "1                           AstraZeneca  ...  ADULT, OLDER_ADULT  PHASE1   \n",
       "2     Radboud University Medical Center  ...               ADULT  PHASE4   \n",
       "3           The University of Hong Kong  ...        CHILD, ADULT     NaN   \n",
       "4  New York State Psychiatric Institute  ...               ADULT  PHASE2   \n",
       "\n",
       "  Enrollment Funder Type      Study Type  \\\n",
       "0      100.0    INDUSTRY  INTERVENTIONAL   \n",
       "1       24.0    INDUSTRY  INTERVENTIONAL   \n",
       "2       30.0       OTHER  INTERVENTIONAL   \n",
       "3      134.0       OTHER   OBSERVATIONAL   \n",
       "4       81.0       OTHER  INTERVENTIONAL   \n",
       "\n",
       "                                        Study Design Start Month  \\\n",
       "0  Allocation: RANDOMIZED|Intervention Model: PAR...          11   \n",
       "1  Allocation: NON_RANDOMIZED|Intervention Model:...           7   \n",
       "2  Allocation: RANDOMIZED|Intervention Model: CRO...           2   \n",
       "3          Observational Model: |Time Perspective: p          -1   \n",
       "4  Allocation: RANDOMIZED|Intervention Model: PAR...           2   \n",
       "\n",
       "  Start Quarter          Condition Category         Conditions_Category  \n",
       "0             4  Other Rare or Unclassified  Other Rare or Unclassified  \n",
       "1             3                    Oncology                    Oncology  \n",
       "2             1  Other Rare or Unclassified  Other Rare or Unclassified  \n",
       "3            -1  Other Rare or Unclassified                Non-Oncology  \n",
       "4             1  Other Rare or Unclassified                Non-Oncology  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.head()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinicaltokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "clinicalmodel = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Unnamed: 0   NCT Number   Study Status  \\\n",
      "0                0  NCT00559130      Completed   \n",
      "1                1  NCT00937664  Not_Completed   \n",
      "2                2  NCT00441597      Completed   \n",
      "3                3  NCT03296228      Completed   \n",
      "4                4  NCT00421603      Completed   \n",
      "...            ...          ...            ...   \n",
      "257572      257572  NCT02360800      Completed   \n",
      "257573      257573  NCT02352506      Completed   \n",
      "257574      257574  NCT04996381      Completed   \n",
      "257575      257575  NCT00380640      Completed   \n",
      "257576      257576  NCT01844336      Completed   \n",
      "\n",
      "                                               Conditions  \\\n",
      "0       Acute Respiratory Distress Syndrome|Acute Lung...   \n",
      "1         Cancer|Solid Tumors|Advanced Solid Malignancies   \n",
      "2       Ischemia Reperfusion Injury|Cardiovascular Dis...   \n",
      "3                         Adolescent Idiopathic Scoliosis   \n",
      "4                                      Cocaine Dependence   \n",
      "...                                                   ...   \n",
      "257572                                         Hemorrhage   \n",
      "257573                                Acute Kidney Injury   \n",
      "257574                Chest X-ray for Clinical Evaluation   \n",
      "257575                              Epidermolysis Bullosa   \n",
      "257576                                Idiopathic Rhinitis   \n",
      "\n",
      "       Other Outcome Measures                               Sponsor  \\\n",
      "0                         NaN            MedaSorb Technologies, Inc   \n",
      "1                         NaN                           AstraZeneca   \n",
      "2                         NaN     Radboud University Medical Center   \n",
      "3                         NaN           The University of Hong Kong   \n",
      "4                         NaN  New York State Psychiatric Institute   \n",
      "...                       ...                                   ...   \n",
      "257572                    NaN                  Cardiochirurgia E.H.   \n",
      "257573                    NaN          Medical University of Vienna   \n",
      "257574                    NaN                     Yonsei University   \n",
      "257575                    NaN        The Hospital for Sick Children   \n",
      "257576                    NaN                      Chordate Medical   \n",
      "\n",
      "                                  Collaborators   Sex  \\\n",
      "0                                           NaN   ALL   \n",
      "1                                           NaN   ALL   \n",
      "2                                        Pfizer  MALE   \n",
      "3                       AO Foundation, AO Spine   ALL   \n",
      "4       National Institute on Drug Abuse (NIDA)   ALL   \n",
      "...                                         ...   ...   \n",
      "257572                                      NaN   ALL   \n",
      "257573                                      NaN   ALL   \n",
      "257574                                      NaN   ALL   \n",
      "257575                                      NaN   ALL   \n",
      "257576                                      NaN   ALL   \n",
      "\n",
      "                              Age  Phases  Enrollment Funder Type  \\\n",
      "0              ADULT, OLDER_ADULT     NaN       100.0    INDUSTRY   \n",
      "1              ADULT, OLDER_ADULT  PHASE1        24.0    INDUSTRY   \n",
      "2                           ADULT  PHASE4        30.0       OTHER   \n",
      "3                    CHILD, ADULT     NaN       134.0       OTHER   \n",
      "4                           ADULT  PHASE2        81.0       OTHER   \n",
      "...                           ...     ...         ...         ...   \n",
      "257572  CHILD, ADULT, OLDER_ADULT     NaN       120.0       OTHER   \n",
      "257573         ADULT, OLDER_ADULT     NaN       500.0       OTHER   \n",
      "257574         ADULT, OLDER_ADULT     NaN       505.0       OTHER   \n",
      "257575               CHILD, ADULT  PHASE2        10.0       OTHER   \n",
      "257576         ADULT, OLDER_ADULT     NaN       208.0    INDUSTRY   \n",
      "\n",
      "            Study Type                                       Study Design  \\\n",
      "0       INTERVENTIONAL  Allocation: RANDOMIZED|Intervention Model: PAR...   \n",
      "1       INTERVENTIONAL  Allocation: NON_RANDOMIZED|Intervention Model:...   \n",
      "2       INTERVENTIONAL  Allocation: RANDOMIZED|Intervention Model: CRO...   \n",
      "3        OBSERVATIONAL          Observational Model: |Time Perspective: p   \n",
      "4       INTERVENTIONAL  Allocation: RANDOMIZED|Intervention Model: PAR...   \n",
      "...                ...                                                ...   \n",
      "257572  INTERVENTIONAL  Allocation: RANDOMIZED|Intervention Model: PAR...   \n",
      "257573   OBSERVATIONAL          Observational Model: |Time Perspective: p   \n",
      "257574   OBSERVATIONAL          Observational Model: |Time Perspective: p   \n",
      "257575  INTERVENTIONAL  Allocation: RANDOMIZED|Intervention Model: CRO...   \n",
      "257576  INTERVENTIONAL  Allocation: RANDOMIZED|Intervention Model: PAR...   \n",
      "\n",
      "        Start Month  Start Quarter          Condition Category  \\\n",
      "0                11              4  Other Rare or Unclassified   \n",
      "1                 7              3                    Oncology   \n",
      "2                 2              1  Other Rare or Unclassified   \n",
      "3                -1             -1  Other Rare or Unclassified   \n",
      "4                 2              1  Other Rare or Unclassified   \n",
      "...             ...            ...                         ...   \n",
      "257572           -1             -1  Other Rare or Unclassified   \n",
      "257573            1              1  Other Rare or Unclassified   \n",
      "257574           -1             -1  Other Rare or Unclassified   \n",
      "257575            9              3  Other Rare or Unclassified   \n",
      "257576           -1             -1  Other Rare or Unclassified   \n",
      "\n",
      "               Conditions_Category  \\\n",
      "0       Other Rare or Unclassified   \n",
      "1                         Oncology   \n",
      "2       Other Rare or Unclassified   \n",
      "3                     Non-Oncology   \n",
      "4                     Non-Oncology   \n",
      "...                            ...   \n",
      "257572                Non-Oncology   \n",
      "257573                Non-Oncology   \n",
      "257574  Other Rare or Unclassified   \n",
      "257575  Other Rare or Unclassified   \n",
      "257576                Non-Oncology   \n",
      "\n",
      "                                             Unstructured  \n",
      "0       The hypothesis of this study is use of CytoSor...  \n",
      "1       The primary purpose of this study is to find o...  \n",
      "2       To study the impact of 3 day exposure to atorv...  \n",
      "3       The purpose of this study is to identify the f...  \n",
      "4       The proposed protocol is a double-blind, place...  \n",
      "...                                                   ...  \n",
      "257572  Bleeding after redo cardiac surgery is a commo...  \n",
      "257573  Acute kidney injury (AKI) is a common complica...  \n",
      "257574  The investigators will develop an artificial i...  \n",
      "257575  The purpose of this study is to assess the eff...  \n",
      "257576  The purpose of this study is to evaluate the p...  \n",
      "\n",
      "[257577 rows x 19 columns]\n"
     ]
    }
   ],
   "source": [
    "# Combine all text attributes into a single column 'Unstructured'\n",
    "data[\"Unstructured\"] = data[\n",
    "    [\"Brief Summary\", \"Study Title\", \"Primary Outcome Measures\", \"Secondary Outcome Measures\"]\n",
    "].astype(str).agg(\" [SEP] \".join, axis=1)\n",
    "\n",
    "# Drop the original text columns\n",
    "data = data.drop(columns=[\"Brief Summary\", \"Study Title\", \"Primary Outcome Measures\", \"Secondary Outcome Measures\"])\n",
    "\n",
    "# Display updated DataFrame\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         The hypothesis of this study is use of CytoSor...\n",
       "1         The primary purpose of this study is to find o...\n",
       "2         To study the impact of 3 day exposure to atorv...\n",
       "3         The purpose of this study is to identify the f...\n",
       "4         The proposed protocol is a double-blind, place...\n",
       "                                ...                        \n",
       "257572    Bleeding after redo cardiac surgery is a commo...\n",
       "257573    Acute kidney injury (AKI) is a common complica...\n",
       "257574    The investigators will develop an artificial i...\n",
       "257575    The purpose of this study is to assess the eff...\n",
       "257576    The purpose of this study is to evaluate the p...\n",
       "Name: Unstructured, Length: 257577, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Unstructured']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "üöÄ Processing chunk 1/1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:   0%|          | 0/1 [00:00<?, ?batch/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 59\u001b[0m\n\u001b[0;32m     56\u001b[0m clinical_text_data \u001b[38;5;241m=\u001b[39m chunk[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnstructured\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# Extract embeddings using batch processing on GPU\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m embeddings_cpu \u001b[38;5;241m=\u001b[39m \u001b[43mclinical_extract_text_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclinical_text_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Returns NumPy array\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# **Fix: Skip saving if embeddings is None (means extraction failed)**\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m embeddings_cpu \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[7], line 21\u001b[0m, in \u001b[0;36mclinical_extract_text_embeddings\u001b[1;34m(text_list, batch_size)\u001b[0m\n\u001b[0;32m     18\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {key: val\u001b[38;5;241m.\u001b[39mto(device, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m key, val \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems()}  \n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():  \u001b[38;5;66;03m# No gradient tracking for inference\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m clinicalmodel(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Compute mean pooling of token embeddings (Sentence Representation)\u001b[39;00m\n\u001b[0;32m     24\u001b[0m batch_embeddings \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# Ensure GPU tensor\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sriha\\anaconda3\\envs\\nest\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\sriha\\anaconda3\\envs\\nest\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\sriha\\anaconda3\\envs\\nest\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1078\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1075\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1076\u001b[0m         token_type_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(input_shape, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m-> 1078\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1080\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1081\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1082\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1083\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1084\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1086\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1087\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones((batch_size, seq_length \u001b[38;5;241m+\u001b[39m past_key_values_length), device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\sriha\\anaconda3\\envs\\nest\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\sriha\\anaconda3\\envs\\nest\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\sriha\\anaconda3\\envs\\nest\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:211\u001b[0m, in \u001b[0;36mBertEmbeddings.forward\u001b[1;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[0;32m    208\u001b[0m         token_type_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(input_shape, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 211\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    212\u001b[0m token_type_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_type_embeddings(token_type_ids)\n\u001b[0;32m    214\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m inputs_embeds \u001b[38;5;241m+\u001b[39m token_type_embeddings\n",
      "File \u001b[1;32mc:\\Users\\sriha\\anaconda3\\envs\\nest\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\sriha\\anaconda3\\envs\\nest\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\sriha\\anaconda3\\envs\\nest\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:190\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sriha\\anaconda3\\envs\\nest\\lib\\site-packages\\torch\\nn\\functional.py:2551\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2545\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2546\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2547\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2548\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2549\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2550\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2551\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Function to extract ClinicalBERT embeddings in GPU-friendly batches\n",
    "def clinical_extract_text_embeddings(text_list, batch_size=512):\n",
    "    \"\"\"Extracts embeddings for text data using ClinicalBERT with batch processing on GPU.\"\"\"\n",
    "    embeddings = []\n",
    "    \n",
    "    dataloader = DataLoader(text_list, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "    for batch_texts in tqdm(dataloader, desc=\"Processing Batches\", unit=\"batch\"):\n",
    "        try:\n",
    "            # Tokenization and moving inputs to GPU\n",
    "            inputs = clinicaltokenizer(batch_texts, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "\n",
    "            # Ensure all tensors are moved to GPU\n",
    "            inputs = {key: val.to(device, non_blocking=True) for key, val in inputs.items()}  \n",
    "\n",
    "            with torch.no_grad():  # No gradient tracking for inference\n",
    "                outputs = clinicalmodel(**inputs)\n",
    "\n",
    "            # Compute mean pooling of token embeddings (Sentence Representation)\n",
    "            batch_embeddings = outputs.last_hidden_state.mean(dim=1).to(device)  # Ensure GPU tensor\n",
    "\n",
    "            embeddings.append(batch_embeddings)\n",
    "\n",
    "            # Free GPU memory after every batch\n",
    "            del inputs, outputs\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        except torch.cuda.OutOfMemoryError:\n",
    "            print(\"\\n‚ö†Ô∏è CUDA OOM Error: Reducing batch size and retrying...\\n\")\n",
    "            torch.cuda.empty_cache()\n",
    "            return clinical_extract_text_embeddings(text_list, batch_size=max(batch_size // 2, 1))  # Reduce batch size & retry\n",
    "\n",
    "    if not embeddings:\n",
    "        print(\"‚ö†Ô∏è No embeddings extracted for this chunk! Skipping...\")\n",
    "        return None  # Return None if extraction failed\n",
    "\n",
    "    return torch.cat([e.to(device) for e in embeddings], dim=0).cpu().numpy()  # Ensure all tensors are on GPU before conversion\n",
    "\n",
    "# Split dataset into 100 chunks\n",
    "num_chunks = 1\n",
    "chunk_size = len(data) // num_chunks\n",
    "\n",
    "for i in range(num_chunks):\n",
    "    print(f\"\\nüöÄ Processing chunk {i+1}/{num_chunks}...\")\n",
    "\n",
    "    # Select subset of data for this chunk\n",
    "    start_idx = i * chunk_size\n",
    "    end_idx = (i + 1) * chunk_size if i != num_chunks - 1 else len(data)  # Last chunk gets remaining data\n",
    "    chunk = data.iloc[start_idx:end_idx].copy()  # Use .copy() to avoid warnings\n",
    "\n",
    "    # Convert text column to list\n",
    "    clinical_text_data = chunk[\"Unstructured\"].astype(str).tolist()\n",
    "\n",
    "    # Extract embeddings using batch processing on GPU\n",
    "    embeddings_cpu = clinical_extract_text_embeddings(clinical_text_data, batch_size=512)  # Returns NumPy array\n",
    "\n",
    "    # **Fix: Skip saving if embeddings is None (means extraction failed)**\n",
    "    if embeddings_cpu is None:\n",
    "        print(f\"‚ö†Ô∏è Skipping chunk {i+1} due to failed embedding extraction.\")\n",
    "        continue  # Skip to the next chunk\n",
    "\n",
    "    # Create DataFrame with embedding columns\n",
    "    clinical_embedding_df = pd.DataFrame(embeddings_cpu, index=chunk.index)\n",
    "    clinical_embedding_df.columns = [f\"Unstructured_embed_{j}\" for j in range(embeddings_cpu.shape[1])]\n",
    "\n",
    "    # Merge embeddings with original text in the chunk\n",
    "    chunk = pd.concat([chunk, clinical_embedding_df], axis=1)\n",
    "\n",
    "    # Save the chunk with both text and embeddings\n",
    "    output_filename = f\"NEST_chunk_clinical_{i+1}.csv\"\n",
    "    chunk.to_csv(output_filename, index=True)\n",
    "    \n",
    "    print(f\"‚úÖ Saved chunk {i+1} to {output_filename}\")\n",
    "\n",
    "    # Free GPU memory after each chunk\n",
    "    del embeddings_cpu, clinical_embedding_df, chunk\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\nüéâ All chunks successfully saved as separate CSV files!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "üöÄ Processing chunk 1/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  17%|‚ñà‚ñã        | 1/6 [00:40<03:21, 40.29s/batch]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling cublasLtMatmul with transpose_mat1 0 transpose_mat2 0 m 768 n 262144 k 768 mat1_ld 768 mat2_ld 768 result_ld 768 abcType 0 computeType 68 scaleType 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 66\u001b[0m\n\u001b[0;32m     63\u001b[0m clinical_text_data \u001b[38;5;241m=\u001b[39m chunk[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnstructured\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# Extract embeddings using batch processing on GPU\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m embeddings_cpu \u001b[38;5;241m=\u001b[39m \u001b[43mclinical_extract_text_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclinical_text_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Returns NumPy array\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# **Fix: Skip saving if embeddings is None (means extraction failed)**\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m embeddings_cpu \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[15], line 25\u001b[0m, in \u001b[0;36mclinical_extract_text_embeddings\u001b[1;34m(text_list, batch_size)\u001b[0m\n\u001b[0;32m     22\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {key: val\u001b[38;5;241m.\u001b[39mto(device, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m key, val \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems()}  \n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():  \u001b[38;5;66;03m# No gradient tracking for inference\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m clinical_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Compute mean pooling of token embeddings (Sentence Representation)\u001b[39;00m\n\u001b[0;32m     28\u001b[0m batch_embeddings \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# Ensure GPU tensor\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sriha\\anaconda3\\envs\\nest\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\sriha\\anaconda3\\envs\\nest\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\sriha\\anaconda3\\envs\\nest\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1142\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1135\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m   1136\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[0;32m   1137\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[0;32m   1138\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[0;32m   1139\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[0;32m   1140\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m-> 1142\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1153\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1154\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1155\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sriha\\anaconda3\\envs\\nest\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\sriha\\anaconda3\\envs\\nest\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\sriha\\anaconda3\\envs\\nest\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:695\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    684\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    685\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    686\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    692\u001b[0m         output_attentions,\n\u001b[0;32m    693\u001b[0m     )\n\u001b[0;32m    694\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 695\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    703\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    706\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\sriha\\anaconda3\\envs\\nest\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\sriha\\anaconda3\\envs\\nest\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\sriha\\anaconda3\\envs\\nest\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:585\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    574\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    575\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    582\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    583\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    584\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 585\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    588\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    589\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    590\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    592\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    594\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sriha\\anaconda3\\envs\\nest\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\sriha\\anaconda3\\envs\\nest\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\sriha\\anaconda3\\envs\\nest\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:524\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    506\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    507\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    513\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    514\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    515\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself(\n\u001b[0;32m    516\u001b[0m         hidden_states,\n\u001b[0;32m    517\u001b[0m         attention_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    522\u001b[0m         output_attentions,\n\u001b[0;32m    523\u001b[0m     )\n\u001b[1;32m--> 524\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mself_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    525\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n\u001b[0;32m    526\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32mc:\\Users\\sriha\\anaconda3\\envs\\nest\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\sriha\\anaconda3\\envs\\nest\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\sriha\\anaconda3\\envs\\nest\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:466\u001b[0m, in \u001b[0;36mBertSelfOutput.forward\u001b[1;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor, input_tensor: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m--> 466\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    467\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[0;32m    468\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_states \u001b[38;5;241m+\u001b[39m input_tensor)\n",
      "File \u001b[1;32mc:\\Users\\sriha\\anaconda3\\envs\\nest\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\sriha\\anaconda3\\envs\\nest\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\sriha\\anaconda3\\envs\\nest\\lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling cublasLtMatmul with transpose_mat1 0 transpose_mat2 0 m 768 n 262144 k 768 mat1_ld 768 mat2_ld 768 result_ld 768 abcType 0 computeType 68 scaleType 0"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load BioBERT model & tokenizer\n",
    "biotokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\")\n",
    "biomodel = AutoModel.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\").to(\"cuda\")  # Move model to GPU\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Function to extract BioBERT embeddings in GPU-friendly batches\n",
    "def bio_extract_text_embeddings(text_list, batch_size=512):\n",
    "    \"\"\"Extracts embeddings for text data using BioBERT with batch processing on GPU.\"\"\"\n",
    "    embeddings = []\n",
    "    \n",
    "    dataloader = DataLoader(text_list, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    for batch_texts in tqdm(dataloader, desc=\"Processing Batches\", unit=\"batch\"):\n",
    "        try:\n",
    "            # Tokenization and moving inputs to GPU\n",
    "            inputs = biotokenizer(batch_texts, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "            inputs = {key: val.to(device) for key, val in inputs.items()}  # Ensure all tensors are on GPU\n",
    "            \n",
    "            with torch.no_grad():  # No gradient tracking for inference\n",
    "                outputs = biomodel(**inputs)\n",
    "\n",
    "            # Compute mean pooling of token embeddings (Sentence Representation)\n",
    "            batch_embeddings = outputs.last_hidden_state.mean(dim=1)  # Stays on GPU\n",
    "\n",
    "            embeddings.append(batch_embeddings)\n",
    "\n",
    "            # Free GPU memory after every batch\n",
    "            del inputs, outputs\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        except torch.cuda.OutOfMemoryError:\n",
    "            print(\"\\n‚ö†Ô∏è CUDA OOM Error: Reducing batch size and retrying...\\n\")\n",
    "            torch.cuda.empty_cache()\n",
    "            return bio_extract_text_embeddings(text_list, batch_size=max(batch_size // 2, 1))  # Reduce batch size & retry\n",
    "\n",
    "    return torch.cat(embeddings, dim=0).cpu().numpy()  # Move embeddings to CPU & convert to NumPy\n",
    "\n",
    "# Split dataset into 100 chunks\n",
    "num_chunks = 100\n",
    "chunk_size = len(data) // num_chunks\n",
    "\n",
    "for i in range(num_chunks):\n",
    "    print(f\"\\nüöÄ Processing chunk {i+1}/{num_chunks}...\")\n",
    "\n",
    "    # Select subset of data for this chunk\n",
    "    start_idx = i * chunk_size\n",
    "    end_idx = (i + 1) * chunk_size if i != num_chunks - 1 else len(data)  # Last chunk gets remaining data\n",
    "    chunk = data.iloc[start_idx:end_idx].copy()  # Use .copy() to avoid warnings\n",
    "\n",
    "    # Convert text column to list\n",
    "    bio_text_data = chunk[\"Unstructured\"].astype(str).tolist()\n",
    "\n",
    "    # Extract embeddings using batch processing on GPU\n",
    "    embeddings_cpu = bio_extract_text_embeddings(bio_text_data, batch_size=512)  # Returns NumPy array\n",
    "\n",
    "    # Create DataFrame with embedding columns\n",
    "    bioembedding_df = pd.DataFrame(embeddings_cpu, index=chunk.index)\n",
    "    bioembedding_df.columns = [f\"Unstructured_embed_{j}\" for j in range(embeddings_cpu.shape[1])]\n",
    "\n",
    "    # Merge embeddings with original text in the chunk\n",
    "    chunk = pd.concat([chunk, bioembedding_df], axis=1)\n",
    "\n",
    "    # Save the chunk with both text and embeddings\n",
    "    output_filename = f\"NEST_chunk_{i+1}.csv\"\n",
    "    chunk.to_csv(output_filename, index=True)\n",
    "    \n",
    "    print(f\"‚úÖ Saved chunk {i+1} to {output_filename}\")\n",
    "\n",
    "    # Free GPU memory after each chunk\n",
    "    del embeddings_cpu, bioembedding_df, chunk\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\nüéâ All chunks successfully saved as separate CSV files!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Merged all chunks into NEST_final_embeddings.csv\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "# Load all chunk files and concatenate\n",
    "files = glob.glob(\"NEST_chunk_clinical*.csv\")\n",
    "df_list = [pd.read_csv(f) for f in files]\n",
    "final_df = pd.concat(df_list, axis=0)\n",
    "final_df.head(10)\n",
    "\n",
    "# Save final combined dataset\n",
    "final_df.to_csv(\"NEST_clinical_embeddings_full.csv\", index=False)\n",
    "print(\"üéØ Merged all chunks into NEST_final_embeddings.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "üöÄ Processing chunk 1/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [02:22<00:00, 23.83s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 1 to NEST_chunk_clinical1.csv\n",
      "\n",
      "üöÄ Processing chunk 2/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [02:23<00:00, 23.86s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 2 to NEST_chunk_clinical2.csv\n",
      "\n",
      "üöÄ Processing chunk 3/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [02:24<00:00, 24.16s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 3 to NEST_chunk_clinical3.csv\n",
      "\n",
      "üöÄ Processing chunk 4/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [02:48<00:00, 28.01s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 4 to NEST_chunk_clinical4.csv\n",
      "\n",
      "üöÄ Processing chunk 5/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [03:01<00:00, 30.27s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 5 to NEST_chunk_clinical5.csv\n",
      "\n",
      "üöÄ Processing chunk 6/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [02:51<00:00, 28.57s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 6 to NEST_chunk_clinical6.csv\n",
      "\n",
      "üöÄ Processing chunk 7/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [02:57<00:00, 29.59s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 7 to NEST_chunk_clinical7.csv\n",
      "\n",
      "üöÄ Processing chunk 8/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [03:12<00:00, 32.07s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 8 to NEST_chunk_clinical8.csv\n",
      "\n",
      "üöÄ Processing chunk 9/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [02:57<00:00, 29.54s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 9 to NEST_chunk_clinical9.csv\n",
      "\n",
      "üöÄ Processing chunk 10/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [02:22<00:00, 23.68s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 10 to NEST_chunk_clinical10.csv\n",
      "\n",
      "üöÄ Processing chunk 11/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [02:22<00:00, 23.78s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 11 to NEST_chunk_clinical11.csv\n",
      "\n",
      "üöÄ Processing chunk 12/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [02:35<00:00, 26.00s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 12 to NEST_chunk_clinical12.csv\n",
      "\n",
      "üöÄ Processing chunk 13/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [02:26<00:00, 24.35s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 13 to NEST_chunk_clinical13.csv\n",
      "\n",
      "üöÄ Processing chunk 14/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [02:21<00:00, 23.63s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 14 to NEST_chunk_clinical14.csv\n",
      "\n",
      "üöÄ Processing chunk 15/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [02:26<00:00, 24.49s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 15 to NEST_chunk_clinical15.csv\n",
      "\n",
      "üöÄ Processing chunk 16/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [02:30<00:00, 25.08s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 16 to NEST_chunk_clinical16.csv\n",
      "\n",
      "üöÄ Processing chunk 17/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [02:32<00:00, 25.45s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 17 to NEST_chunk_clinical17.csv\n",
      "\n",
      "üöÄ Processing chunk 18/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [02:30<00:00, 25.02s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 18 to NEST_chunk_clinical18.csv\n",
      "\n",
      "üöÄ Processing chunk 19/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [02:30<00:00, 25.04s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 19 to NEST_chunk_clinical19.csv\n",
      "\n",
      "üöÄ Processing chunk 20/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [02:29<00:00, 24.96s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 20 to NEST_chunk_clinical20.csv\n",
      "\n",
      "üöÄ Processing chunk 21/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [02:29<00:00, 24.96s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 21 to NEST_chunk_clinical21.csv\n",
      "\n",
      "üöÄ Processing chunk 22/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [02:29<00:00, 24.90s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 22 to NEST_chunk_clinical22.csv\n",
      "\n",
      "üöÄ Processing chunk 23/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [02:29<00:00, 24.97s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 23 to NEST_chunk_clinical23.csv\n",
      "\n",
      "üöÄ Processing chunk 24/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [02:29<00:00, 24.86s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 24 to NEST_chunk_clinical24.csv\n",
      "\n",
      "üöÄ Processing chunk 25/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [02:30<00:00, 25.00s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 25 to NEST_chunk_clinical25.csv\n",
      "\n",
      "üöÄ Processing chunk 26/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [02:30<00:00, 25.05s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 26 to NEST_chunk_clinical26.csv\n",
      "\n",
      "üöÄ Processing chunk 27/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [02:23<00:00, 23.94s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 27 to NEST_chunk_clinical27.csv\n",
      "\n",
      "üöÄ Processing chunk 28/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [02:16<00:00, 22.67s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 28 to NEST_chunk_clinical28.csv\n",
      "\n",
      "üöÄ Processing chunk 29/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [02:16<00:00, 22.68s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 29 to NEST_chunk_clinical29.csv\n",
      "\n",
      "üöÄ Processing chunk 30/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [02:15<00:00, 22.65s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 30 to NEST_chunk_clinical30.csv\n",
      "\n",
      "üöÄ Processing chunk 31/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [02:16<00:00, 22.71s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 31 to NEST_chunk_clinical31.csv\n",
      "\n",
      "üöÄ Processing chunk 32/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [02:15<00:00, 22.66s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 32 to NEST_chunk_clinical32.csv\n",
      "\n",
      "üöÄ Processing chunk 33/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [02:15<00:00, 22.61s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 33 to NEST_chunk_clinical33.csv\n",
      "\n",
      "üöÄ Processing chunk 34/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [02:16<00:00, 22.68s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 34 to NEST_chunk_clinical34.csv\n",
      "\n",
      "üöÄ Processing chunk 35/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [02:15<00:00, 22.61s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 35 to NEST_chunk_clinical35.csv\n",
      "\n",
      "üöÄ Processing chunk 36/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [02:15<00:00, 22.62s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 36 to NEST_chunk_clinical36.csv\n",
      "\n",
      "üöÄ Processing chunk 37/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [02:16<00:00, 22.67s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 37 to NEST_chunk_clinical37.csv\n",
      "\n",
      "üöÄ Processing chunk 38/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [02:15<00:00, 22.66s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 38 to NEST_chunk_clinical38.csv\n",
      "\n",
      "üöÄ Processing chunk 39/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [02:15<00:00, 22.63s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 39 to NEST_chunk_clinical39.csv\n",
      "\n",
      "üöÄ Processing chunk 40/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [02:15<00:00, 22.61s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 40 to NEST_chunk_clinical40.csv\n",
      "\n",
      "üöÄ Processing chunk 41/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [02:15<00:00, 22.63s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 41 to NEST_chunk_clinical41.csv\n",
      "\n",
      "üöÄ Processing chunk 42/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [02:15<00:00, 22.63s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 42 to NEST_chunk_clinical42.csv\n",
      "\n",
      "üöÄ Processing chunk 43/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [02:15<00:00, 22.64s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 43 to NEST_chunk_clinical43.csv\n",
      "\n",
      "üöÄ Processing chunk 44/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [02:15<00:00, 22.63s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 44 to NEST_chunk_clinical44.csv\n",
      "\n",
      "üöÄ Processing chunk 45/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [02:15<00:00, 22.62s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 45 to NEST_chunk_clinical45.csv\n",
      "\n",
      "üöÄ Processing chunk 46/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [02:15<00:00, 22.64s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 46 to NEST_chunk_clinical46.csv\n",
      "\n",
      "üöÄ Processing chunk 47/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [02:15<00:00, 22.58s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 47 to NEST_chunk_clinical47.csv\n",
      "\n",
      "üöÄ Processing chunk 48/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [02:15<00:00, 22.64s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 48 to NEST_chunk_clinical48.csv\n",
      "\n",
      "üöÄ Processing chunk 49/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [02:16<00:00, 22.73s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 49 to NEST_chunk_clinical49.csv\n",
      "\n",
      "üöÄ Processing chunk 50/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [02:15<00:00, 22.62s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 50 to NEST_chunk_clinical50.csv\n",
      "\n",
      "üöÄ Processing chunk 51/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [02:16<00:00, 22.79s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 51 to NEST_chunk_clinical51.csv\n",
      "\n",
      "üöÄ Processing chunk 52/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [02:15<00:00, 22.63s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 52 to NEST_chunk_clinical52.csv\n",
      "\n",
      "üöÄ Processing chunk 53/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [02:15<00:00, 22.62s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 53 to NEST_chunk_clinical53.csv\n",
      "\n",
      "üöÄ Processing chunk 54/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [02:16<00:00, 22.67s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 54 to NEST_chunk_clinical54.csv\n",
      "\n",
      "üöÄ Processing chunk 55/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [02:15<00:00, 22.65s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 55 to NEST_chunk_clinical55.csv\n",
      "\n",
      "üöÄ Processing chunk 56/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [02:15<00:00, 22.61s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 56 to NEST_chunk_clinical56.csv\n",
      "\n",
      "üöÄ Processing chunk 57/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [02:15<00:00, 22.64s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 57 to NEST_chunk_clinical57.csv\n",
      "\n",
      "üöÄ Processing chunk 58/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [02:15<00:00, 22.63s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 58 to NEST_chunk_clinical58.csv\n",
      "\n",
      "üöÄ Processing chunk 59/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [02:15<00:00, 22.61s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 59 to NEST_chunk_clinical59.csv\n",
      "\n",
      "üöÄ Processing chunk 60/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [02:15<00:00, 22.55s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 60 to NEST_chunk_clinical60.csv\n",
      "\n",
      "üöÄ Processing chunk 61/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [02:15<00:00, 22.61s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 61 to NEST_chunk_clinical61.csv\n",
      "\n",
      "üöÄ Processing chunk 62/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [02:15<00:00, 22.60s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 62 to NEST_chunk_clinical62.csv\n",
      "\n",
      "üöÄ Processing chunk 63/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [02:16<00:00, 22.69s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 63 to NEST_chunk_clinical63.csv\n",
      "\n",
      "üöÄ Processing chunk 64/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [02:15<00:00, 22.61s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 64 to NEST_chunk_clinical64.csv\n",
      "\n",
      "üöÄ Processing chunk 65/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [02:15<00:00, 22.61s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 65 to NEST_chunk_clinical65.csv\n",
      "\n",
      "üöÄ Processing chunk 66/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [02:15<00:00, 22.57s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 66 to NEST_chunk_clinical66.csv\n",
      "\n",
      "üöÄ Processing chunk 67/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [02:15<00:00, 22.64s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 67 to NEST_chunk_clinical67.csv\n",
      "\n",
      "üöÄ Processing chunk 68/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [02:15<00:00, 22.63s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 68 to NEST_chunk_clinical68.csv\n",
      "\n",
      "üöÄ Processing chunk 69/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [02:15<00:00, 22.59s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 69 to NEST_chunk_clinical69.csv\n",
      "\n",
      "üöÄ Processing chunk 70/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [02:15<00:00, 22.59s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 70 to NEST_chunk_clinical70.csv\n",
      "\n",
      "üöÄ Processing chunk 71/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [02:15<00:00, 22.61s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 71 to NEST_chunk_clinical71.csv\n",
      "\n",
      "üöÄ Processing chunk 72/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [02:16<00:00, 22.72s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 72 to NEST_chunk_clinical72.csv\n",
      "\n",
      "üöÄ Processing chunk 73/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [02:26<00:00, 24.40s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 73 to NEST_chunk_clinical73.csv\n",
      "\n",
      "üöÄ Processing chunk 74/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [02:51<00:00, 28.66s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 74 to NEST_chunk_clinical74.csv\n",
      "\n",
      "üöÄ Processing chunk 75/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [03:07<00:00, 31.32s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 75 to NEST_chunk_clinical75.csv\n",
      "\n",
      "üöÄ Processing chunk 76/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [03:08<00:00, 31.50s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 76 to NEST_chunk_clinical76.csv\n",
      "\n",
      "üöÄ Processing chunk 77/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [03:08<00:00, 31.36s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 77 to NEST_chunk_clinical77.csv\n",
      "\n",
      "üöÄ Processing chunk 78/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [03:06<00:00, 31.08s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 78 to NEST_chunk_clinical78.csv\n",
      "\n",
      "üöÄ Processing chunk 79/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [03:07<00:00, 31.31s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 79 to NEST_chunk_clinical79.csv\n",
      "\n",
      "üöÄ Processing chunk 80/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [03:07<00:00, 31.26s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 80 to NEST_chunk_clinical80.csv\n",
      "\n",
      "üöÄ Processing chunk 81/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [03:08<00:00, 31.39s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 81 to NEST_chunk_clinical81.csv\n",
      "\n",
      "üöÄ Processing chunk 82/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [03:08<00:00, 31.49s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 82 to NEST_chunk_clinical82.csv\n",
      "\n",
      "üöÄ Processing chunk 83/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [03:09<00:00, 31.57s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 83 to NEST_chunk_clinical83.csv\n",
      "\n",
      "üöÄ Processing chunk 84/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [03:09<00:00, 31.61s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 84 to NEST_chunk_clinical84.csv\n",
      "\n",
      "üöÄ Processing chunk 85/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [03:08<00:00, 31.41s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 85 to NEST_chunk_clinical85.csv\n",
      "\n",
      "üöÄ Processing chunk 86/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [02:40<00:00, 26.68s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 86 to NEST_chunk_clinical86.csv\n",
      "\n",
      "üöÄ Processing chunk 87/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [02:15<00:00, 22.62s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 87 to NEST_chunk_clinical87.csv\n",
      "\n",
      "üöÄ Processing chunk 88/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [02:15<00:00, 22.63s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 88 to NEST_chunk_clinical88.csv\n",
      "\n",
      "üöÄ Processing chunk 89/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [02:15<00:00, 22.66s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 89 to NEST_chunk_clinical89.csv\n",
      "\n",
      "üöÄ Processing chunk 90/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [02:15<00:00, 22.64s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 90 to NEST_chunk_clinical90.csv\n",
      "\n",
      "üöÄ Processing chunk 91/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [02:15<00:00, 22.58s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 91 to NEST_chunk_clinical91.csv\n",
      "\n",
      "üöÄ Processing chunk 92/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [02:15<00:00, 22.63s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 92 to NEST_chunk_clinical92.csv\n",
      "\n",
      "üöÄ Processing chunk 93/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [02:15<00:00, 22.63s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 93 to NEST_chunk_clinical93.csv\n",
      "\n",
      "üöÄ Processing chunk 94/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [02:15<00:00, 22.62s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 94 to NEST_chunk_clinical94.csv\n",
      "\n",
      "üöÄ Processing chunk 95/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [02:15<00:00, 22.65s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 95 to NEST_chunk_clinical95.csv\n",
      "\n",
      "üöÄ Processing chunk 96/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [02:15<00:00, 22.62s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 96 to NEST_chunk_clinical96.csv\n",
      "\n",
      "üöÄ Processing chunk 97/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [02:15<00:00, 22.59s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 97 to NEST_chunk_clinical97.csv\n",
      "\n",
      "üöÄ Processing chunk 98/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [02:15<00:00, 22.63s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 98 to NEST_chunk_clinical98.csv\n",
      "\n",
      "üöÄ Processing chunk 99/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [02:16<00:00, 22.69s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 99 to NEST_chunk_clinical99.csv\n",
      "\n",
      "üöÄ Processing chunk 100/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [02:16<00:00, 22.82s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk 100 to NEST_chunk_clinical100.csv\n",
      "\n",
      "üéâ All chunks successfully saved as separate CSV files!\n"
     ]
    }
   ],
   "source": [
    "# Load BioBERT model & tokenizer\n",
    "clinicaltokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "clinicalmodel = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\").to(\"cuda\")  # Move model to GPU\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Function to extract BioBERT embeddings in GPU-friendly batches\n",
    "def clinical_extract_text_embeddings(text_list, batch_size=512):\n",
    "    \"\"\"Extracts embeddings for text data using BioBERT with batch processing on GPU.\"\"\"\n",
    "    embeddings = []\n",
    "    \n",
    "    dataloader = DataLoader(text_list, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    for batch_texts in tqdm(dataloader, desc=\"Processing Batches\", unit=\"batch\"):\n",
    "        try:\n",
    "            # Tokenization and moving inputs to GPU\n",
    "            inputs = clinicaltokenizer(batch_texts, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "            inputs = {key: val.to(device) for key, val in inputs.items()}  # Ensure all tensors are on GPU\n",
    "            \n",
    "            with torch.no_grad():  # No gradient tracking for inference\n",
    "                outputs = clinicalmodel(**inputs)\n",
    "\n",
    "            # Compute mean pooling of token embeddings (Sentence Representation)\n",
    "            batch_embeddings = outputs.last_hidden_state.mean(dim=1)  # Stays on GPU\n",
    "\n",
    "            embeddings.append(batch_embeddings)\n",
    "\n",
    "            # Free GPU memory after every batch\n",
    "            del inputs, outputs\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        except torch.cuda.OutOfMemoryError:\n",
    "            print(\"\\n‚ö†Ô∏è CUDA OOM Error: Reducing batch size and retrying...\\n\")\n",
    "            torch.cuda.empty_cache()\n",
    "            return clinical_extract_text_embeddings(text_list, batch_size=max(batch_size // 2, 1))  # Reduce batch size & retry\n",
    "\n",
    "    return torch.cat(embeddings, dim=0).cpu().numpy()  # Move embeddings to CPU & convert to NumPy\n",
    "\n",
    "# Split dataset into 100 chunks\n",
    "num_chunks = 100\n",
    "chunk_size = len(data) // num_chunks\n",
    "\n",
    "for i in range(num_chunks):\n",
    "    print(f\"\\nüöÄ Processing chunk {i+1}/{num_chunks}...\")\n",
    "\n",
    "    # Select subset of data for this chunk\n",
    "    start_idx = i * chunk_size\n",
    "    end_idx = (i + 1) * chunk_size if i != num_chunks - 1 else len(data)  # Last chunk gets remaining data\n",
    "    chunk = data.iloc[start_idx:end_idx].copy()  # Use .copy() to avoid warnings\n",
    "\n",
    "    # Convert text column to list\n",
    "    clinical_text_data = chunk[\"Unstructured\"].astype(str).tolist()\n",
    "\n",
    "    # Extract embeddings using batch processing on GPU\n",
    "    embeddings_cpu = clinical_extract_text_embeddings(clinical_text_data, batch_size=512)  # Returns NumPy array\n",
    "\n",
    "    # Create DataFrame with embedding columns\n",
    "    clinicalembedding_df = pd.DataFrame(embeddings_cpu, index=chunk.index)\n",
    "    clinicalembedding_df.columns = [f\"Unstructured_embed_{j}\" for j in range(embeddings_cpu.shape[1])]\n",
    "\n",
    "    # Merge embeddings with original text in the chunk\n",
    "    chunk = pd.concat([chunk, clinicalembedding_df], axis=1)\n",
    "\n",
    "    # Save the chunk with both text and embeddings\n",
    "    output_filename = f\"NEST_chunk_clinical{i+1}.csv\"\n",
    "    chunk.to_csv(output_filename, index=True)\n",
    "    \n",
    "    print(f\"‚úÖ Saved chunk {i+1} to {output_filename}\")\n",
    "\n",
    "    # Free GPU memory after each chunk\n",
    "    del embeddings_cpu, clinicalembedding_df, chunk\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\nüéâ All chunks successfully saved as separate CSV files!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

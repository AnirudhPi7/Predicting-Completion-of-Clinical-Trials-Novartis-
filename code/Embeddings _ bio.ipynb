{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from pytorch_tabular.models.tab_transformer import TabTransformerModel\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 257577 entries, 0 to 257576\n",
      "Data columns (total 22 columns):\n",
      " #   Column                      Non-Null Count   Dtype  \n",
      "---  ------                      --------------   -----  \n",
      " 0   Unnamed: 0                  257577 non-null  int64  \n",
      " 1   NCT Number                  257577 non-null  object \n",
      " 2   Study Title                 257577 non-null  object \n",
      " 3   Study Status                257577 non-null  object \n",
      " 4   Brief Summary               257577 non-null  object \n",
      " 5   Conditions                  257577 non-null  object \n",
      " 6   Primary Outcome Measures    247086 non-null  object \n",
      " 7   Secondary Outcome Measures  185779 non-null  object \n",
      " 8   Other Outcome Measures      18272 non-null   object \n",
      " 9   Sponsor                     257577 non-null  object \n",
      " 10  Collaborators               83679 non-null   object \n",
      " 11  Sex                         257317 non-null  object \n",
      " 12  Age                         257577 non-null  object \n",
      " 13  Phases                      112765 non-null  object \n",
      " 14  Enrollment                  254205 non-null  float64\n",
      " 15  Funder Type                 257577 non-null  object \n",
      " 16  Study Type                  257577 non-null  object \n",
      " 17  Study Design                257577 non-null  object \n",
      " 18  Start Month                 257577 non-null  int64  \n",
      " 19  Start Quarter               257577 non-null  int64  \n",
      " 20  Condition Category          257577 non-null  object \n",
      " 21  Conditions_Category         257577 non-null  object \n",
      "dtypes: float64(1), int64(3), object(18)\n",
      "memory usage: 43.2+ MB\n",
      "Initial Dataset Info:\n",
      " None\n",
      "\n",
      "Sample Data:\n",
      "    Unnamed: 0   NCT Number                                        Study Title  \\\n",
      "0           0  NCT00559130  Efficacy Study of CytoSorb Hemoperfusion Devic...   \n",
      "1           1  NCT00937664  Safety and Tolerability Study of AZD7762 in Co...   \n",
      "2           2  NCT00441597  Does Atorvastatin Reduce Ischemia-Reperfusion ...   \n",
      "3           3  NCT03296228  Comparison of Dynamic Radiographs in Determini...   \n",
      "4           4  NCT00421603  A Placebo-Controlled Study of Mixed Amphetamin...   \n",
      "\n",
      "    Study Status                                      Brief Summary  \\\n",
      "0      Completed  The hypothesis of this study is use of CytoSor...   \n",
      "1  Not_Completed  The primary purpose of this study is to find o...   \n",
      "2      Completed  To study the impact of 3 day exposure to atorv...   \n",
      "3      Completed  The purpose of this study is to identify the f...   \n",
      "4      Completed  The proposed protocol is a double-blind, place...   \n",
      "\n",
      "                                          Conditions  \\\n",
      "0  Acute Respiratory Distress Syndrome|Acute Lung...   \n",
      "1    Cancer|Solid Tumors|Advanced Solid Malignancies   \n",
      "2  Ischemia Reperfusion Injury|Cardiovascular Dis...   \n",
      "3                    Adolescent Idiopathic Scoliosis   \n",
      "4                                 Cocaine Dependence   \n",
      "\n",
      "                            Primary Outcome Measures  \\\n",
      "0  Relative IL-6 levels as a percent (%) of basel...   \n",
      "1  Assessment of adverse events (based on CTCAE v...   \n",
      "2  Annexin A 5 targeting in the non dominant then...   \n",
      "3  Investigate the flexibility equivalence of dif...   \n",
      "4  Three Weeks of Continuous Cocaine Abstinence a...   \n",
      "\n",
      "                          Secondary Outcome Measures Other Outcome Measures  \\\n",
      "0  Ventilator Free Days, Reduction cytokines TNF-...                    NaN   \n",
      "1  Pharmacokinetic effect of AZD7762 when adminis...                    NaN   \n",
      "2  workload during ischemic exercise, workload du...                    NaN   \n",
      "3  Incorporate these findings into the Lenke Clas...                    NaN   \n",
      "4                                                NaN                    NaN   \n",
      "\n",
      "                                Sponsor  ...                 Age  Phases  \\\n",
      "0            MedaSorb Technologies, Inc  ...  ADULT, OLDER_ADULT     NaN   \n",
      "1                           AstraZeneca  ...  ADULT, OLDER_ADULT  PHASE1   \n",
      "2     Radboud University Medical Center  ...               ADULT  PHASE4   \n",
      "3           The University of Hong Kong  ...        CHILD, ADULT     NaN   \n",
      "4  New York State Psychiatric Institute  ...               ADULT  PHASE2   \n",
      "\n",
      "  Enrollment Funder Type      Study Type  \\\n",
      "0      100.0    INDUSTRY  INTERVENTIONAL   \n",
      "1       24.0    INDUSTRY  INTERVENTIONAL   \n",
      "2       30.0       OTHER  INTERVENTIONAL   \n",
      "3      134.0       OTHER   OBSERVATIONAL   \n",
      "4       81.0       OTHER  INTERVENTIONAL   \n",
      "\n",
      "                                        Study Design Start Month  \\\n",
      "0  Allocation: RANDOMIZED|Intervention Model: PAR...          11   \n",
      "1  Allocation: NON_RANDOMIZED|Intervention Model:...           7   \n",
      "2  Allocation: RANDOMIZED|Intervention Model: CRO...           2   \n",
      "3          Observational Model: |Time Perspective: p          -1   \n",
      "4  Allocation: RANDOMIZED|Intervention Model: PAR...           2   \n",
      "\n",
      "  Start Quarter          Condition Category         Conditions_Category  \n",
      "0             4  Other Rare or Unclassified  Other Rare or Unclassified  \n",
      "1             3                    Oncology                    Oncology  \n",
      "2             1  Other Rare or Unclassified  Other Rare or Unclassified  \n",
      "3            -1  Other Rare or Unclassified                Non-Oncology  \n",
      "4             1  Other Rare or Unclassified                Non-Oncology  \n",
      "\n",
      "[5 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load Dataset\n",
    "def load_data(file_path):\n",
    "    \"\"\"Loads the dataset and provides an initial overview.\"\"\"\n",
    "    data = pd.read_csv(file_path)\n",
    "    print(\"Initial Dataset Info:\\n\", data.info())\n",
    "    print(\"\\nSample Data:\\n\", data.head())\n",
    "    return data\n",
    "data = load_data(\"C:\\\\Users\\\\sriha\\\\Music\\\\Case Comps\\\\NEST\\\\Data\\\\category_updated.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'NCT Number', 'Study Title', 'Study Status',\n",
       "       'Brief Summary', 'Conditions', 'Primary Outcome Measures',\n",
       "       'Secondary Outcome Measures', 'Other Outcome Measures', 'Sponsor',\n",
       "       'Collaborators', 'Sex', 'Age', 'Phases', 'Enrollment', 'Funder Type',\n",
       "       'Study Type', 'Study Design', 'Start Month', 'Start Quarter',\n",
       "       'Condition Category', 'Conditions_Category'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data= data.head(5)\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Extracts embeddings for text data using the specified BERT model.\"\"\"\n",
    "biotokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\")\n",
    "biomodel = AutoModel.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bio_extract_text_embeddings(text_list, batch_size=512):\n",
    "    \"\"\"Extracts embeddings for text data using BioBERT with batch processing on GPU.\"\"\"\n",
    "    embeddings = []\n",
    "    \n",
    "    dataloader = DataLoader(text_list, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    for batch_texts in tqdm(dataloader, desc=\"Processing Batches\", unit=\"batch\"):\n",
    "        inputs = biotokenizer(batch_texts, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(device)\n",
    "        \n",
    "        with torch.no_grad():  # No gradient tracking for inference\n",
    "            outputs = biomodel(**inputs)\n",
    "        \n",
    "        # Compute mean pooling of token embeddings (Sentence Representation)\n",
    "        batch_embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "        embeddings.append(batch_embeddings)\n",
    "    \n",
    "    return np.vstack(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Unnamed: 0   NCT Number   Study Status  \\\n",
      "0                0  NCT00559130      Completed   \n",
      "1                1  NCT00937664  Not_Completed   \n",
      "2                2  NCT00441597      Completed   \n",
      "3                3  NCT03296228      Completed   \n",
      "4                4  NCT00421603      Completed   \n",
      "...            ...          ...            ...   \n",
      "257572      257572  NCT02360800      Completed   \n",
      "257573      257573  NCT02352506      Completed   \n",
      "257574      257574  NCT04996381      Completed   \n",
      "257575      257575  NCT00380640      Completed   \n",
      "257576      257576  NCT01844336      Completed   \n",
      "\n",
      "                                               Conditions  \\\n",
      "0       Acute Respiratory Distress Syndrome|Acute Lung...   \n",
      "1         Cancer|Solid Tumors|Advanced Solid Malignancies   \n",
      "2       Ischemia Reperfusion Injury|Cardiovascular Dis...   \n",
      "3                         Adolescent Idiopathic Scoliosis   \n",
      "4                                      Cocaine Dependence   \n",
      "...                                                   ...   \n",
      "257572                                         Hemorrhage   \n",
      "257573                                Acute Kidney Injury   \n",
      "257574                Chest X-ray for Clinical Evaluation   \n",
      "257575                              Epidermolysis Bullosa   \n",
      "257576                                Idiopathic Rhinitis   \n",
      "\n",
      "       Other Outcome Measures                               Sponsor  \\\n",
      "0                         NaN            MedaSorb Technologies, Inc   \n",
      "1                         NaN                           AstraZeneca   \n",
      "2                         NaN     Radboud University Medical Center   \n",
      "3                         NaN           The University of Hong Kong   \n",
      "4                         NaN  New York State Psychiatric Institute   \n",
      "...                       ...                                   ...   \n",
      "257572                    NaN                  Cardiochirurgia E.H.   \n",
      "257573                    NaN          Medical University of Vienna   \n",
      "257574                    NaN                     Yonsei University   \n",
      "257575                    NaN        The Hospital for Sick Children   \n",
      "257576                    NaN                      Chordate Medical   \n",
      "\n",
      "                                  Collaborators   Sex  \\\n",
      "0                                           NaN   ALL   \n",
      "1                                           NaN   ALL   \n",
      "2                                        Pfizer  MALE   \n",
      "3                       AO Foundation, AO Spine   ALL   \n",
      "4       National Institute on Drug Abuse (NIDA)   ALL   \n",
      "...                                         ...   ...   \n",
      "257572                                      NaN   ALL   \n",
      "257573                                      NaN   ALL   \n",
      "257574                                      NaN   ALL   \n",
      "257575                                      NaN   ALL   \n",
      "257576                                      NaN   ALL   \n",
      "\n",
      "                              Age  Phases  Enrollment Funder Type  \\\n",
      "0              ADULT, OLDER_ADULT     NaN       100.0    INDUSTRY   \n",
      "1              ADULT, OLDER_ADULT  PHASE1        24.0    INDUSTRY   \n",
      "2                           ADULT  PHASE4        30.0       OTHER   \n",
      "3                    CHILD, ADULT     NaN       134.0       OTHER   \n",
      "4                           ADULT  PHASE2        81.0       OTHER   \n",
      "...                           ...     ...         ...         ...   \n",
      "257572  CHILD, ADULT, OLDER_ADULT     NaN       120.0       OTHER   \n",
      "257573         ADULT, OLDER_ADULT     NaN       500.0       OTHER   \n",
      "257574         ADULT, OLDER_ADULT     NaN       505.0       OTHER   \n",
      "257575               CHILD, ADULT  PHASE2        10.0       OTHER   \n",
      "257576         ADULT, OLDER_ADULT     NaN       208.0    INDUSTRY   \n",
      "\n",
      "            Study Type                                       Study Design  \\\n",
      "0       INTERVENTIONAL  Allocation: RANDOMIZED|Intervention Model: PAR...   \n",
      "1       INTERVENTIONAL  Allocation: NON_RANDOMIZED|Intervention Model:...   \n",
      "2       INTERVENTIONAL  Allocation: RANDOMIZED|Intervention Model: CRO...   \n",
      "3        OBSERVATIONAL          Observational Model: |Time Perspective: p   \n",
      "4       INTERVENTIONAL  Allocation: RANDOMIZED|Intervention Model: PAR...   \n",
      "...                ...                                                ...   \n",
      "257572  INTERVENTIONAL  Allocation: RANDOMIZED|Intervention Model: PAR...   \n",
      "257573   OBSERVATIONAL          Observational Model: |Time Perspective: p   \n",
      "257574   OBSERVATIONAL          Observational Model: |Time Perspective: p   \n",
      "257575  INTERVENTIONAL  Allocation: RANDOMIZED|Intervention Model: CRO...   \n",
      "257576  INTERVENTIONAL  Allocation: RANDOMIZED|Intervention Model: PAR...   \n",
      "\n",
      "        Start Month  Start Quarter          Condition Category  \\\n",
      "0                11              4  Other Rare or Unclassified   \n",
      "1                 7              3                    Oncology   \n",
      "2                 2              1  Other Rare or Unclassified   \n",
      "3                -1             -1  Other Rare or Unclassified   \n",
      "4                 2              1  Other Rare or Unclassified   \n",
      "...             ...            ...                         ...   \n",
      "257572           -1             -1  Other Rare or Unclassified   \n",
      "257573            1              1  Other Rare or Unclassified   \n",
      "257574           -1             -1  Other Rare or Unclassified   \n",
      "257575            9              3  Other Rare or Unclassified   \n",
      "257576           -1             -1  Other Rare or Unclassified   \n",
      "\n",
      "               Conditions_Category  \\\n",
      "0       Other Rare or Unclassified   \n",
      "1                         Oncology   \n",
      "2       Other Rare or Unclassified   \n",
      "3                     Non-Oncology   \n",
      "4                     Non-Oncology   \n",
      "...                            ...   \n",
      "257572                Non-Oncology   \n",
      "257573                Non-Oncology   \n",
      "257574  Other Rare or Unclassified   \n",
      "257575  Other Rare or Unclassified   \n",
      "257576                Non-Oncology   \n",
      "\n",
      "                                             Unstructured  \n",
      "0       The hypothesis of this study is use of CytoSor...  \n",
      "1       The primary purpose of this study is to find o...  \n",
      "2       To study the impact of 3 day exposure to atorv...  \n",
      "3       The purpose of this study is to identify the f...  \n",
      "4       The proposed protocol is a double-blind, place...  \n",
      "...                                                   ...  \n",
      "257572  Bleeding after redo cardiac surgery is a commo...  \n",
      "257573  Acute kidney injury (AKI) is a common complica...  \n",
      "257574  The investigators will develop an artificial i...  \n",
      "257575  The purpose of this study is to assess the eff...  \n",
      "257576  The purpose of this study is to evaluate the p...  \n",
      "\n",
      "[257577 rows x 19 columns]\n"
     ]
    }
   ],
   "source": [
    "# Combine all text attributes into a single column 'Unstructured'\n",
    "data[\"Unstructured\"] = data[\n",
    "    [\"Brief Summary\", \"Study Title\", \"Primary Outcome Measures\", \"Secondary Outcome Measures\"]\n",
    "].astype(str).agg(\" [SEP] \".join, axis=1)\n",
    "\n",
    "# Drop the original text columns\n",
    "data = data.drop(columns=[\"Brief Summary\", \"Study Title\", \"Primary Outcome Measures\", \"Secondary Outcome Measures\"])\n",
    "\n",
    "# Display updated DataFrame\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sriha\\anaconda3\\envs\\nest\\lib\\site-packages\\torch\\cuda\\memory.py:192\u001b[0m, in \u001b[0;36mempty_cache\u001b[1;34m()\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Release all unoccupied cached memory currently held by the caching\u001b[39;00m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;124;03mallocator so that those can be used in other GPU application and visible in\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;124;03m`nvidia-smi`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;124;03m    more details about GPU memory management.\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_initialized():\n\u001b[1;32m--> 192\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_emptyCache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data =data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "ðŸš€ Processing chunk 1/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:22<00:00, 23.70s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 1 to NEST_chunk_1.csv\n",
      "\n",
      "ðŸš€ Processing chunk 2/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:19<00:00, 23.25s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 2 to NEST_chunk_2.csv\n",
      "\n",
      "ðŸš€ Processing chunk 3/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:19<00:00, 23.18s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 3 to NEST_chunk_3.csv\n",
      "\n",
      "ðŸš€ Processing chunk 4/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:19<00:00, 23.19s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 4 to NEST_chunk_4.csv\n",
      "\n",
      "ðŸš€ Processing chunk 5/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:18<00:00, 23.14s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 5 to NEST_chunk_5.csv\n",
      "\n",
      "ðŸš€ Processing chunk 6/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:18<00:00, 23.15s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 6 to NEST_chunk_6.csv\n",
      "\n",
      "ðŸš€ Processing chunk 7/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:18<00:00, 23.13s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 7 to NEST_chunk_7.csv\n",
      "\n",
      "ðŸš€ Processing chunk 8/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:20<00:00, 23.46s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 8 to NEST_chunk_8.csv\n",
      "\n",
      "ðŸš€ Processing chunk 9/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:18<00:00, 23.14s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 9 to NEST_chunk_9.csv\n",
      "\n",
      "ðŸš€ Processing chunk 10/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:18<00:00, 23.13s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 10 to NEST_chunk_10.csv\n",
      "\n",
      "ðŸš€ Processing chunk 11/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:19<00:00, 23.25s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 11 to NEST_chunk_11.csv\n",
      "\n",
      "ðŸš€ Processing chunk 12/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:18<00:00, 23.13s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 12 to NEST_chunk_12.csv\n",
      "\n",
      "ðŸš€ Processing chunk 13/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:19<00:00, 23.22s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 13 to NEST_chunk_13.csv\n",
      "\n",
      "ðŸš€ Processing chunk 14/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:19<00:00, 23.18s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 14 to NEST_chunk_14.csv\n",
      "\n",
      "ðŸš€ Processing chunk 15/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:19<00:00, 23.18s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 15 to NEST_chunk_15.csv\n",
      "\n",
      "ðŸš€ Processing chunk 16/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:19<00:00, 23.18s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 16 to NEST_chunk_16.csv\n",
      "\n",
      "ðŸš€ Processing chunk 17/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:19<00:00, 23.23s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 17 to NEST_chunk_17.csv\n",
      "\n",
      "ðŸš€ Processing chunk 18/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:18<00:00, 23.16s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 18 to NEST_chunk_18.csv\n",
      "\n",
      "ðŸš€ Processing chunk 19/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:18<00:00, 23.13s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 19 to NEST_chunk_19.csv\n",
      "\n",
      "ðŸš€ Processing chunk 20/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [16:49<00:00, 168.24s/batch]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 20 to NEST_chunk_20.csv\n",
      "\n",
      "ðŸš€ Processing chunk 21/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:19<00:00, 23.20s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 21 to NEST_chunk_21.csv\n",
      "\n",
      "ðŸš€ Processing chunk 22/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:19<00:00, 23.24s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 22 to NEST_chunk_22.csv\n",
      "\n",
      "ðŸš€ Processing chunk 23/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:19<00:00, 23.24s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 23 to NEST_chunk_23.csv\n",
      "\n",
      "ðŸš€ Processing chunk 24/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:19<00:00, 23.18s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 24 to NEST_chunk_24.csv\n",
      "\n",
      "ðŸš€ Processing chunk 25/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:20<00:00, 23.35s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 25 to NEST_chunk_25.csv\n",
      "\n",
      "ðŸš€ Processing chunk 26/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:19<00:00, 23.19s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 26 to NEST_chunk_26.csv\n",
      "\n",
      "ðŸš€ Processing chunk 27/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:19<00:00, 23.18s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 27 to NEST_chunk_27.csv\n",
      "\n",
      "ðŸš€ Processing chunk 28/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:19<00:00, 23.20s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 28 to NEST_chunk_28.csv\n",
      "\n",
      "ðŸš€ Processing chunk 29/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:19<00:00, 23.21s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 29 to NEST_chunk_29.csv\n",
      "\n",
      "ðŸš€ Processing chunk 30/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:19<00:00, 23.24s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 30 to NEST_chunk_30.csv\n",
      "\n",
      "ðŸš€ Processing chunk 31/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:19<00:00, 23.23s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 31 to NEST_chunk_31.csv\n",
      "\n",
      "ðŸš€ Processing chunk 32/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:19<00:00, 23.18s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 32 to NEST_chunk_32.csv\n",
      "\n",
      "ðŸš€ Processing chunk 33/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:19<00:00, 23.22s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 33 to NEST_chunk_33.csv\n",
      "\n",
      "ðŸš€ Processing chunk 34/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:19<00:00, 23.17s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 34 to NEST_chunk_34.csv\n",
      "\n",
      "ðŸš€ Processing chunk 35/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [12:26<00:00, 124.45s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 35 to NEST_chunk_35.csv\n",
      "\n",
      "ðŸš€ Processing chunk 36/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:22<00:00, 23.78s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 36 to NEST_chunk_36.csv\n",
      "\n",
      "ðŸš€ Processing chunk 37/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:19<00:00, 23.20s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 37 to NEST_chunk_37.csv\n",
      "\n",
      "ðŸš€ Processing chunk 38/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:19<00:00, 23.21s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 38 to NEST_chunk_38.csv\n",
      "\n",
      "ðŸš€ Processing chunk 39/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:19<00:00, 23.21s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 39 to NEST_chunk_39.csv\n",
      "\n",
      "ðŸš€ Processing chunk 40/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:19<00:00, 23.21s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 40 to NEST_chunk_40.csv\n",
      "\n",
      "ðŸš€ Processing chunk 41/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:19<00:00, 23.19s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 41 to NEST_chunk_41.csv\n",
      "\n",
      "ðŸš€ Processing chunk 42/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [29:44<00:00, 297.44s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 42 to NEST_chunk_42.csv\n",
      "\n",
      "ðŸš€ Processing chunk 43/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:19<00:00, 23.18s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 43 to NEST_chunk_43.csv\n",
      "\n",
      "ðŸš€ Processing chunk 44/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:19<00:00, 23.23s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 44 to NEST_chunk_44.csv\n",
      "\n",
      "ðŸš€ Processing chunk 45/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:19<00:00, 23.30s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 45 to NEST_chunk_45.csv\n",
      "\n",
      "ðŸš€ Processing chunk 46/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:19<00:00, 23.22s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 46 to NEST_chunk_46.csv\n",
      "\n",
      "ðŸš€ Processing chunk 47/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:19<00:00, 23.21s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 47 to NEST_chunk_47.csv\n",
      "\n",
      "ðŸš€ Processing chunk 48/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:19<00:00, 23.22s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 48 to NEST_chunk_48.csv\n",
      "\n",
      "ðŸš€ Processing chunk 49/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:19<00:00, 23.21s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 49 to NEST_chunk_49.csv\n",
      "\n",
      "ðŸš€ Processing chunk 50/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:21<00:00, 23.56s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 50 to NEST_chunk_50.csv\n",
      "\n",
      "ðŸš€ Processing chunk 51/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:18<00:00, 23.16s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 51 to NEST_chunk_51.csv\n",
      "\n",
      "ðŸš€ Processing chunk 52/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [5:36:34<00:00, 3365.80s/batch]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 52 to NEST_chunk_52.csv\n",
      "\n",
      "ðŸš€ Processing chunk 53/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:19<00:00, 23.21s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 53 to NEST_chunk_53.csv\n",
      "\n",
      "ðŸš€ Processing chunk 54/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:19<00:00, 23.18s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 54 to NEST_chunk_54.csv\n",
      "\n",
      "ðŸš€ Processing chunk 55/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:18<00:00, 23.16s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 55 to NEST_chunk_55.csv\n",
      "\n",
      "ðŸš€ Processing chunk 56/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:19<00:00, 23.20s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 56 to NEST_chunk_56.csv\n",
      "\n",
      "ðŸš€ Processing chunk 57/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:19<00:00, 23.18s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 57 to NEST_chunk_57.csv\n",
      "\n",
      "ðŸš€ Processing chunk 58/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:19<00:00, 23.19s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 58 to NEST_chunk_58.csv\n",
      "\n",
      "ðŸš€ Processing chunk 59/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:19<00:00, 23.20s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 59 to NEST_chunk_59.csv\n",
      "\n",
      "ðŸš€ Processing chunk 60/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:18<00:00, 23.15s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 60 to NEST_chunk_60.csv\n",
      "\n",
      "ðŸš€ Processing chunk 61/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:18<00:00, 23.16s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 61 to NEST_chunk_61.csv\n",
      "\n",
      "ðŸš€ Processing chunk 62/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:19<00:00, 23.18s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 62 to NEST_chunk_62.csv\n",
      "\n",
      "ðŸš€ Processing chunk 63/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:19<00:00, 23.18s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 63 to NEST_chunk_63.csv\n",
      "\n",
      "ðŸš€ Processing chunk 64/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [12:07<00:00, 121.24s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 64 to NEST_chunk_64.csv\n",
      "\n",
      "ðŸš€ Processing chunk 65/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:19<00:00, 23.28s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 65 to NEST_chunk_65.csv\n",
      "\n",
      "ðŸš€ Processing chunk 66/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:19<00:00, 23.20s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 66 to NEST_chunk_66.csv\n",
      "\n",
      "ðŸš€ Processing chunk 67/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:18<00:00, 23.17s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 67 to NEST_chunk_67.csv\n",
      "\n",
      "ðŸš€ Processing chunk 68/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:18<00:00, 23.17s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 68 to NEST_chunk_68.csv\n",
      "\n",
      "ðŸš€ Processing chunk 69/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:19<00:00, 23.19s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 69 to NEST_chunk_69.csv\n",
      "\n",
      "ðŸš€ Processing chunk 70/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:18<00:00, 23.13s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 70 to NEST_chunk_70.csv\n",
      "\n",
      "ðŸš€ Processing chunk 71/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [11:22<00:00, 113.76s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 71 to NEST_chunk_71.csv\n",
      "\n",
      "ðŸš€ Processing chunk 72/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:19<00:00, 23.21s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 72 to NEST_chunk_72.csv\n",
      "\n",
      "ðŸš€ Processing chunk 73/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:19<00:00, 23.18s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 73 to NEST_chunk_73.csv\n",
      "\n",
      "ðŸš€ Processing chunk 74/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:20<00:00, 23.35s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 74 to NEST_chunk_74.csv\n",
      "\n",
      "ðŸš€ Processing chunk 75/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:19<00:00, 23.19s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 75 to NEST_chunk_75.csv\n",
      "\n",
      "ðŸš€ Processing chunk 76/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:19<00:00, 23.19s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 76 to NEST_chunk_76.csv\n",
      "\n",
      "ðŸš€ Processing chunk 77/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:19<00:00, 23.18s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 77 to NEST_chunk_77.csv\n",
      "\n",
      "ðŸš€ Processing chunk 78/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [20:35<00:00, 205.94s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 78 to NEST_chunk_78.csv\n",
      "\n",
      "ðŸš€ Processing chunk 79/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:19<00:00, 23.22s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 79 to NEST_chunk_79.csv\n",
      "\n",
      "ðŸš€ Processing chunk 80/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:19<00:00, 23.24s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 80 to NEST_chunk_80.csv\n",
      "\n",
      "ðŸš€ Processing chunk 81/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:19<00:00, 23.27s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 81 to NEST_chunk_81.csv\n",
      "\n",
      "ðŸš€ Processing chunk 82/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:19<00:00, 23.20s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 82 to NEST_chunk_82.csv\n",
      "\n",
      "ðŸš€ Processing chunk 83/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:19<00:00, 23.18s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 83 to NEST_chunk_83.csv\n",
      "\n",
      "ðŸš€ Processing chunk 84/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:19<00:00, 23.22s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 84 to NEST_chunk_84.csv\n",
      "\n",
      "ðŸš€ Processing chunk 85/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:19<00:00, 23.17s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 85 to NEST_chunk_85.csv\n",
      "\n",
      "ðŸš€ Processing chunk 86/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:24<00:00, 24.07s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 86 to NEST_chunk_86.csv\n",
      "\n",
      "ðŸš€ Processing chunk 87/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:16<00:00, 22.77s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 87 to NEST_chunk_87.csv\n",
      "\n",
      "ðŸš€ Processing chunk 88/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:16<00:00, 22.72s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 88 to NEST_chunk_88.csv\n",
      "\n",
      "ðŸš€ Processing chunk 89/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:16<00:00, 22.74s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 89 to NEST_chunk_89.csv\n",
      "\n",
      "ðŸš€ Processing chunk 90/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:16<00:00, 22.71s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 90 to NEST_chunk_90.csv\n",
      "\n",
      "ðŸš€ Processing chunk 91/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:16<00:00, 22.74s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 91 to NEST_chunk_91.csv\n",
      "\n",
      "ðŸš€ Processing chunk 92/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:16<00:00, 22.74s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 92 to NEST_chunk_92.csv\n",
      "\n",
      "ðŸš€ Processing chunk 93/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:18<00:00, 23.08s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 93 to NEST_chunk_93.csv\n",
      "\n",
      "ðŸš€ Processing chunk 94/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:22<00:00, 23.73s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 94 to NEST_chunk_94.csv\n",
      "\n",
      "ðŸš€ Processing chunk 95/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:21<00:00, 23.60s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 95 to NEST_chunk_95.csv\n",
      "\n",
      "ðŸš€ Processing chunk 96/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:18<00:00, 23.05s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 96 to NEST_chunk_96.csv\n",
      "\n",
      "ðŸš€ Processing chunk 97/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:19<00:00, 23.30s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 97 to NEST_chunk_97.csv\n",
      "\n",
      "ðŸš€ Processing chunk 98/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:16<00:00, 22.82s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 98 to NEST_chunk_98.csv\n",
      "\n",
      "ðŸš€ Processing chunk 99/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:16<00:00, 22.76s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 99 to NEST_chunk_99.csv\n",
      "\n",
      "ðŸš€ Processing chunk 100/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [02:17<00:00, 22.84s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunk 100 to NEST_chunk_100.csv\n",
      "\n",
      "ðŸŽ‰ All chunks successfully saved as separate CSV files!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load BioBERT model & tokenizer\n",
    "biotokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\")\n",
    "biomodel = AutoModel.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\").to(\"cuda\")  # Move model to GPU\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Function to extract BioBERT embeddings in GPU-friendly batches\n",
    "def bio_extract_text_embeddings(text_list, batch_size=512):\n",
    "    \"\"\"Extracts embeddings for text data using BioBERT with batch processing on GPU.\"\"\"\n",
    "    embeddings = []\n",
    "    \n",
    "    dataloader = DataLoader(text_list, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    for batch_texts in tqdm(dataloader, desc=\"Processing Batches\", unit=\"batch\"):\n",
    "        try:\n",
    "            # Tokenization and moving inputs to GPU\n",
    "            inputs = biotokenizer(batch_texts, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "            inputs = {key: val.to(device) for key, val in inputs.items()}  # Ensure all tensors are on GPU\n",
    "            \n",
    "            with torch.no_grad():  # No gradient tracking for inference\n",
    "                outputs = biomodel(**inputs)\n",
    "\n",
    "            # Compute mean pooling of token embeddings (Sentence Representation)\n",
    "            batch_embeddings = outputs.last_hidden_state.mean(dim=1)  # Stays on GPU\n",
    "\n",
    "            embeddings.append(batch_embeddings)\n",
    "\n",
    "            # Free GPU memory after every batch\n",
    "            del inputs, outputs\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        except torch.cuda.OutOfMemoryError:\n",
    "            print(\"\\nâš ï¸ CUDA OOM Error: Reducing batch size and retrying...\\n\")\n",
    "            torch.cuda.empty_cache()\n",
    "            return bio_extract_text_embeddings(text_list, batch_size=max(batch_size // 2, 1))  # Reduce batch size & retry\n",
    "\n",
    "    return torch.cat(embeddings, dim=0).cpu().numpy()  # Move embeddings to CPU & convert to NumPy\n",
    "\n",
    "# Split dataset into 100 chunks\n",
    "num_chunks = 100\n",
    "chunk_size = len(data) // num_chunks\n",
    "\n",
    "for i in range(num_chunks):\n",
    "    print(f\"\\nðŸš€ Processing chunk {i+1}/{num_chunks}...\")\n",
    "\n",
    "    # Select subset of data for this chunk\n",
    "    start_idx = i * chunk_size\n",
    "    end_idx = (i + 1) * chunk_size if i != num_chunks - 1 else len(data)  # Last chunk gets remaining data\n",
    "    chunk = data.iloc[start_idx:end_idx].copy()  # Use .copy() to avoid warnings\n",
    "\n",
    "    # Convert text column to list\n",
    "    bio_text_data = chunk[\"Unstructured\"].astype(str).tolist()\n",
    "\n",
    "    # Extract embeddings using batch processing on GPU\n",
    "    embeddings_cpu = bio_extract_text_embeddings(bio_text_data, batch_size=512)  # Returns NumPy array\n",
    "\n",
    "    # Create DataFrame with embedding columns\n",
    "    bioembedding_df = pd.DataFrame(embeddings_cpu, index=chunk.index)\n",
    "    bioembedding_df.columns = [f\"Unstructured_embed_{j}\" for j in range(embeddings_cpu.shape[1])]\n",
    "\n",
    "    # Merge embeddings with original text in the chunk\n",
    "    chunk = pd.concat([chunk, bioembedding_df], axis=1)\n",
    "\n",
    "    # Save the chunk with both text and embeddings\n",
    "    output_filename = f\"NEST_chunk_{i+1}.csv\"\n",
    "    chunk.to_csv(output_filename, index=True)\n",
    "    \n",
    "    print(f\"âœ… Saved chunk {i+1} to {output_filename}\")\n",
    "\n",
    "    # Free GPU memory after each chunk\n",
    "    del embeddings_cpu, bioembedding_df, chunk\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\nðŸŽ‰ All chunks successfully saved as separate CSV files!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unstructured_embed_0</th>\n",
       "      <th>Unstructured_embed_1</th>\n",
       "      <th>Unstructured_embed_2</th>\n",
       "      <th>Unstructured_embed_3</th>\n",
       "      <th>Unstructured_embed_4</th>\n",
       "      <th>Unstructured_embed_5</th>\n",
       "      <th>Unstructured_embed_6</th>\n",
       "      <th>Unstructured_embed_7</th>\n",
       "      <th>Unstructured_embed_8</th>\n",
       "      <th>...</th>\n",
       "      <th>Unstructured_embed_758</th>\n",
       "      <th>Unstructured_embed_759</th>\n",
       "      <th>Unstructured_embed_760</th>\n",
       "      <th>Unstructured_embed_761</th>\n",
       "      <th>Unstructured_embed_762</th>\n",
       "      <th>Unstructured_embed_763</th>\n",
       "      <th>Unstructured_embed_764</th>\n",
       "      <th>Unstructured_embed_765</th>\n",
       "      <th>Unstructured_embed_766</th>\n",
       "      <th>Unstructured_embed_767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.173068</td>\n",
       "      <td>-0.077015</td>\n",
       "      <td>-0.525726</td>\n",
       "      <td>-0.037300</td>\n",
       "      <td>-0.203847</td>\n",
       "      <td>0.030538</td>\n",
       "      <td>-0.040831</td>\n",
       "      <td>0.19147</td>\n",
       "      <td>0.245301</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.253266</td>\n",
       "      <td>0.127309</td>\n",
       "      <td>-0.11036</td>\n",
       "      <td>0.348753</td>\n",
       "      <td>-0.166995</td>\n",
       "      <td>0.212118</td>\n",
       "      <td>0.242659</td>\n",
       "      <td>-0.229306</td>\n",
       "      <td>-0.052761</td>\n",
       "      <td>-0.388725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.199231</td>\n",
       "      <td>-0.065837</td>\n",
       "      <td>-0.115439</td>\n",
       "      <td>-0.000676</td>\n",
       "      <td>-0.122404</td>\n",
       "      <td>0.003565</td>\n",
       "      <td>-0.092257</td>\n",
       "      <td>-0.12714</td>\n",
       "      <td>0.089963</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.047347</td>\n",
       "      <td>0.167718</td>\n",
       "      <td>0.06174</td>\n",
       "      <td>0.211448</td>\n",
       "      <td>0.024834</td>\n",
       "      <td>0.416763</td>\n",
       "      <td>0.272169</td>\n",
       "      <td>-0.318060</td>\n",
       "      <td>0.294743</td>\n",
       "      <td>-0.060007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.173068</td>\n",
       "      <td>-0.077015</td>\n",
       "      <td>-0.525726</td>\n",
       "      <td>-0.037300</td>\n",
       "      <td>-0.203847</td>\n",
       "      <td>0.030538</td>\n",
       "      <td>-0.040831</td>\n",
       "      <td>0.19147</td>\n",
       "      <td>0.245301</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.253266</td>\n",
       "      <td>0.127309</td>\n",
       "      <td>-0.11036</td>\n",
       "      <td>0.348753</td>\n",
       "      <td>-0.166995</td>\n",
       "      <td>0.212118</td>\n",
       "      <td>0.242659</td>\n",
       "      <td>-0.229306</td>\n",
       "      <td>-0.052761</td>\n",
       "      <td>-0.388725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-0.199231</td>\n",
       "      <td>-0.065837</td>\n",
       "      <td>-0.115439</td>\n",
       "      <td>-0.000676</td>\n",
       "      <td>-0.122404</td>\n",
       "      <td>0.003565</td>\n",
       "      <td>-0.092257</td>\n",
       "      <td>-0.12714</td>\n",
       "      <td>0.089963</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.047347</td>\n",
       "      <td>0.167718</td>\n",
       "      <td>0.06174</td>\n",
       "      <td>0.211448</td>\n",
       "      <td>0.024834</td>\n",
       "      <td>0.416763</td>\n",
       "      <td>0.272169</td>\n",
       "      <td>-0.318060</td>\n",
       "      <td>0.294743</td>\n",
       "      <td>-0.060007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-0.173068</td>\n",
       "      <td>-0.077015</td>\n",
       "      <td>-0.525726</td>\n",
       "      <td>-0.037300</td>\n",
       "      <td>-0.203847</td>\n",
       "      <td>0.030538</td>\n",
       "      <td>-0.040831</td>\n",
       "      <td>0.19147</td>\n",
       "      <td>0.245301</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.253266</td>\n",
       "      <td>0.127309</td>\n",
       "      <td>-0.11036</td>\n",
       "      <td>0.348753</td>\n",
       "      <td>-0.166995</td>\n",
       "      <td>0.212118</td>\n",
       "      <td>0.242659</td>\n",
       "      <td>-0.229306</td>\n",
       "      <td>-0.052761</td>\n",
       "      <td>-0.388725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1360</th>\n",
       "      <td>1360</td>\n",
       "      <td>-0.173068</td>\n",
       "      <td>-0.077015</td>\n",
       "      <td>-0.525726</td>\n",
       "      <td>-0.037300</td>\n",
       "      <td>-0.203847</td>\n",
       "      <td>0.030538</td>\n",
       "      <td>-0.040831</td>\n",
       "      <td>0.19147</td>\n",
       "      <td>0.245301</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.253266</td>\n",
       "      <td>0.127309</td>\n",
       "      <td>-0.11036</td>\n",
       "      <td>0.348753</td>\n",
       "      <td>-0.166995</td>\n",
       "      <td>0.212118</td>\n",
       "      <td>0.242659</td>\n",
       "      <td>-0.229306</td>\n",
       "      <td>-0.052761</td>\n",
       "      <td>-0.388725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1361</th>\n",
       "      <td>1361</td>\n",
       "      <td>-0.199231</td>\n",
       "      <td>-0.065837</td>\n",
       "      <td>-0.115439</td>\n",
       "      <td>-0.000676</td>\n",
       "      <td>-0.122404</td>\n",
       "      <td>0.003565</td>\n",
       "      <td>-0.092257</td>\n",
       "      <td>-0.12714</td>\n",
       "      <td>0.089963</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.047347</td>\n",
       "      <td>0.167718</td>\n",
       "      <td>0.06174</td>\n",
       "      <td>0.211448</td>\n",
       "      <td>0.024834</td>\n",
       "      <td>0.416763</td>\n",
       "      <td>0.272169</td>\n",
       "      <td>-0.318060</td>\n",
       "      <td>0.294743</td>\n",
       "      <td>-0.060007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1362</th>\n",
       "      <td>1362</td>\n",
       "      <td>-0.173068</td>\n",
       "      <td>-0.077015</td>\n",
       "      <td>-0.525726</td>\n",
       "      <td>-0.037300</td>\n",
       "      <td>-0.203847</td>\n",
       "      <td>0.030538</td>\n",
       "      <td>-0.040831</td>\n",
       "      <td>0.19147</td>\n",
       "      <td>0.245301</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.253266</td>\n",
       "      <td>0.127309</td>\n",
       "      <td>-0.11036</td>\n",
       "      <td>0.348753</td>\n",
       "      <td>-0.166995</td>\n",
       "      <td>0.212118</td>\n",
       "      <td>0.242659</td>\n",
       "      <td>-0.229306</td>\n",
       "      <td>-0.052761</td>\n",
       "      <td>-0.388725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1363</th>\n",
       "      <td>1363</td>\n",
       "      <td>-0.199231</td>\n",
       "      <td>-0.065837</td>\n",
       "      <td>-0.115439</td>\n",
       "      <td>-0.000676</td>\n",
       "      <td>-0.122404</td>\n",
       "      <td>0.003565</td>\n",
       "      <td>-0.092257</td>\n",
       "      <td>-0.12714</td>\n",
       "      <td>0.089963</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.047347</td>\n",
       "      <td>0.167718</td>\n",
       "      <td>0.06174</td>\n",
       "      <td>0.211448</td>\n",
       "      <td>0.024834</td>\n",
       "      <td>0.416763</td>\n",
       "      <td>0.272169</td>\n",
       "      <td>-0.318060</td>\n",
       "      <td>0.294743</td>\n",
       "      <td>-0.060007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1364</th>\n",
       "      <td>1364</td>\n",
       "      <td>-0.173068</td>\n",
       "      <td>-0.077015</td>\n",
       "      <td>-0.525726</td>\n",
       "      <td>-0.037300</td>\n",
       "      <td>-0.203847</td>\n",
       "      <td>0.030538</td>\n",
       "      <td>-0.040831</td>\n",
       "      <td>0.19147</td>\n",
       "      <td>0.245301</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.253266</td>\n",
       "      <td>0.127309</td>\n",
       "      <td>-0.11036</td>\n",
       "      <td>0.348753</td>\n",
       "      <td>-0.166995</td>\n",
       "      <td>0.212118</td>\n",
       "      <td>0.242659</td>\n",
       "      <td>-0.229306</td>\n",
       "      <td>-0.052761</td>\n",
       "      <td>-0.388725</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1365 rows Ã— 769 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  Unstructured_embed_0  Unstructured_embed_1  \\\n",
       "0              0             -0.173068             -0.077015   \n",
       "1              1             -0.199231             -0.065837   \n",
       "2              2             -0.173068             -0.077015   \n",
       "3              3             -0.199231             -0.065837   \n",
       "4              4             -0.173068             -0.077015   \n",
       "...          ...                   ...                   ...   \n",
       "1360        1360             -0.173068             -0.077015   \n",
       "1361        1361             -0.199231             -0.065837   \n",
       "1362        1362             -0.173068             -0.077015   \n",
       "1363        1363             -0.199231             -0.065837   \n",
       "1364        1364             -0.173068             -0.077015   \n",
       "\n",
       "      Unstructured_embed_2  Unstructured_embed_3  Unstructured_embed_4  \\\n",
       "0                -0.525726             -0.037300             -0.203847   \n",
       "1                -0.115439             -0.000676             -0.122404   \n",
       "2                -0.525726             -0.037300             -0.203847   \n",
       "3                -0.115439             -0.000676             -0.122404   \n",
       "4                -0.525726             -0.037300             -0.203847   \n",
       "...                    ...                   ...                   ...   \n",
       "1360             -0.525726             -0.037300             -0.203847   \n",
       "1361             -0.115439             -0.000676             -0.122404   \n",
       "1362             -0.525726             -0.037300             -0.203847   \n",
       "1363             -0.115439             -0.000676             -0.122404   \n",
       "1364             -0.525726             -0.037300             -0.203847   \n",
       "\n",
       "      Unstructured_embed_5  Unstructured_embed_6  Unstructured_embed_7  \\\n",
       "0                 0.030538             -0.040831               0.19147   \n",
       "1                 0.003565             -0.092257              -0.12714   \n",
       "2                 0.030538             -0.040831               0.19147   \n",
       "3                 0.003565             -0.092257              -0.12714   \n",
       "4                 0.030538             -0.040831               0.19147   \n",
       "...                    ...                   ...                   ...   \n",
       "1360              0.030538             -0.040831               0.19147   \n",
       "1361              0.003565             -0.092257              -0.12714   \n",
       "1362              0.030538             -0.040831               0.19147   \n",
       "1363              0.003565             -0.092257              -0.12714   \n",
       "1364              0.030538             -0.040831               0.19147   \n",
       "\n",
       "      Unstructured_embed_8  ...  Unstructured_embed_758  \\\n",
       "0                 0.245301  ...               -0.253266   \n",
       "1                 0.089963  ...               -0.047347   \n",
       "2                 0.245301  ...               -0.253266   \n",
       "3                 0.089963  ...               -0.047347   \n",
       "4                 0.245301  ...               -0.253266   \n",
       "...                    ...  ...                     ...   \n",
       "1360              0.245301  ...               -0.253266   \n",
       "1361              0.089963  ...               -0.047347   \n",
       "1362              0.245301  ...               -0.253266   \n",
       "1363              0.089963  ...               -0.047347   \n",
       "1364              0.245301  ...               -0.253266   \n",
       "\n",
       "      Unstructured_embed_759  Unstructured_embed_760  Unstructured_embed_761  \\\n",
       "0                   0.127309                -0.11036                0.348753   \n",
       "1                   0.167718                 0.06174                0.211448   \n",
       "2                   0.127309                -0.11036                0.348753   \n",
       "3                   0.167718                 0.06174                0.211448   \n",
       "4                   0.127309                -0.11036                0.348753   \n",
       "...                      ...                     ...                     ...   \n",
       "1360                0.127309                -0.11036                0.348753   \n",
       "1361                0.167718                 0.06174                0.211448   \n",
       "1362                0.127309                -0.11036                0.348753   \n",
       "1363                0.167718                 0.06174                0.211448   \n",
       "1364                0.127309                -0.11036                0.348753   \n",
       "\n",
       "      Unstructured_embed_762  Unstructured_embed_763  Unstructured_embed_764  \\\n",
       "0                  -0.166995                0.212118                0.242659   \n",
       "1                   0.024834                0.416763                0.272169   \n",
       "2                  -0.166995                0.212118                0.242659   \n",
       "3                   0.024834                0.416763                0.272169   \n",
       "4                  -0.166995                0.212118                0.242659   \n",
       "...                      ...                     ...                     ...   \n",
       "1360               -0.166995                0.212118                0.242659   \n",
       "1361                0.024834                0.416763                0.272169   \n",
       "1362               -0.166995                0.212118                0.242659   \n",
       "1363                0.024834                0.416763                0.272169   \n",
       "1364               -0.166995                0.212118                0.242659   \n",
       "\n",
       "      Unstructured_embed_765  Unstructured_embed_766  Unstructured_embed_767  \n",
       "0                  -0.229306               -0.052761               -0.388725  \n",
       "1                  -0.318060                0.294743               -0.060007  \n",
       "2                  -0.229306               -0.052761               -0.388725  \n",
       "3                  -0.318060                0.294743               -0.060007  \n",
       "4                  -0.229306               -0.052761               -0.388725  \n",
       "...                      ...                     ...                     ...  \n",
       "1360               -0.229306               -0.052761               -0.388725  \n",
       "1361               -0.318060                0.294743               -0.060007  \n",
       "1362               -0.229306               -0.052761               -0.388725  \n",
       "1363               -0.318060                0.294743               -0.060007  \n",
       "1364               -0.229306               -0.052761               -0.388725  \n",
       "\n",
       "[1365 rows x 769 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bioembedding_df_1 = pd.read_csv('NEST_embed_chunk_1.csv',)\n",
    "bioembedding_df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Load BioBERT model & tokenizer\u001b[39;00m\n\u001b[0;32m      9\u001b[0m biotokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdmis-lab/biobert-base-cased-v1.1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m biomodel \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdmis-lab/biobert-base-cased-v1.1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Move model to GPU\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Check if GPU is available\u001b[39;00m\n\u001b[0;32m     13\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\sriha\\anaconda3\\envs\\nest\\lib\\site-packages\\transformers\\modeling_utils.py:3110\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3105\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[0;32m   3106\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   3107\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3108\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3109\u001b[0m         )\n\u001b[1;32m-> 3110\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\sriha\\anaconda3\\envs\\nest\\lib\\site-packages\\torch\\nn\\modules\\module.py:1340\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1337\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1338\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m-> 1340\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sriha\\anaconda3\\envs\\nest\\lib\\site-packages\\torch\\nn\\modules\\module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sriha\\anaconda3\\envs\\nest\\lib\\site-packages\\torch\\nn\\modules\\module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sriha\\anaconda3\\envs\\nest\\lib\\site-packages\\torch\\nn\\modules\\module.py:927\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    923\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    924\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    925\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    926\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 927\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    928\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    930\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sriha\\anaconda3\\envs\\nest\\lib\\site-packages\\torch\\nn\\modules\\module.py:1326\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m   1320\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[0;32m   1321\u001b[0m             device,\n\u001b[0;32m   1322\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1323\u001b[0m             non_blocking,\n\u001b[0;32m   1324\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[0;32m   1325\u001b[0m         )\n\u001b[1;32m-> 1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1331\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load BioBERT model & tokenizer\n",
    "biotokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\")\n",
    "biomodel = AutoModel.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\").to(\"cuda\")  # Move model to GPU\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Function to extract BioBERT embeddings in GPU-friendly batches with memory control\n",
    "def bio_extract_text_embeddings(text_list, batch_size=256):\n",
    "    \"\"\"Extracts embeddings for text data using BioBERT with batch processing on GPU while handling OOM errors.\"\"\"\n",
    "    embeddings = []\n",
    "    \n",
    "    dataloader = DataLoader(text_list, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    for batch_texts in tqdm(dataloader, desc=\"Processing Batches\", unit=\"batch\"):\n",
    "        try:\n",
    "            # Tokenization and moving inputs to GPU\n",
    "            inputs = biotokenizer(batch_texts, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "            inputs = {key: val.to(device) for key, val in inputs.items()}  # Move all tensors to GPU\n",
    "\n",
    "            with torch.no_grad():  # No gradient tracking for inference\n",
    "                outputs = biomodel(**inputs)\n",
    "\n",
    "            # Compute mean pooling of token embeddings (Sentence Representation)\n",
    "            batch_embeddings = outputs.last_hidden_state.mean(dim=1)  # Keep on GPU\n",
    "            \n",
    "            embeddings.append(batch_embeddings)\n",
    "\n",
    "            # Free GPU memory after each batch\n",
    "            del inputs, outputs\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        except torch.cuda.OutOfMemoryError:\n",
    "            print(\"\\nâš ï¸ CUDA OOM Error: Reducing batch size to avoid crash.\\n\")\n",
    "            torch.cuda.empty_cache()\n",
    "            return bio_extract_text_embeddings(text_list, batch_size=max(batch_size // 2, 1))  # Reduce batch size & retry\n",
    "\n",
    "    return torch.cat(embeddings, dim=0).cpu().numpy()  # Ensure output is on CPU\n",
    "\n",
    "# Split dataset into 15 chunks\n",
    "num_chunks = 15\n",
    "chunk_size = len(data) // num_chunks\n",
    "\n",
    "for i in range(num_chunks):\n",
    "    print(f\"\\nðŸš€ Processing chunk {i+1}/{num_chunks}...\")\n",
    "\n",
    "    # Select subset of data for this chunk\n",
    "    start_idx = i * chunk_size\n",
    "    end_idx = (i + 1) * chunk_size if i != num_chunks - 1 else len(data)  # Last chunk gets remaining data\n",
    "    chunk = data.iloc[start_idx:end_idx]\n",
    "\n",
    "    # Convert text column to list\n",
    "    bio_text_data = chunk[\"Unstructured\"].astype(str).tolist()\n",
    "\n",
    "    # Extract embeddings using batch processing on GPU\n",
    "    embeddings_cpu = bio_extract_text_embeddings(bio_text_data, batch_size=256)  # Dynamically adjusts batch size\n",
    "\n",
    "    # Create DataFrame with embedding columns\n",
    "    bioembedding_df = pd.DataFrame(embeddings_cpu, index=chunk.index)\n",
    "    bioembedding_df.columns = [f\"Unstructured_embed_{i}\" for i in range(embeddings_cpu.shape[1])]\n",
    "\n",
    "    # Save each chunk separately to avoid memory overload\n",
    "    output_filename = f\"NEST_embed_chunk_{i+1}.csv\"\n",
    "    bioembedding_df.to_csv(output_filename, index=True)\n",
    "    \n",
    "    print(f\"âœ… Saved chunk {i+1} to {output_filename}\")\n",
    "\n",
    "    # Free GPU memory after each chunk\n",
    "    del embeddings_cpu, bioembedding_df\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\nðŸŽ‰ All embeddings successfully saved in separate CSV files!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Processing attribute: Unstructured\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:   0%|          | 0/49 [00:01<?, ?batch/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 45\u001b[0m\n\u001b[0;32m     42\u001b[0m bio_text_data \u001b[38;5;241m=\u001b[39m data[attribute]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Extract embeddings using batch processing on GPU\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m embeddings_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mbio_extract_text_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbio_text_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Returns tensor on GPU\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Move embeddings back to CPU for DataFrame creation\u001b[39;00m\n\u001b[0;32m     48\u001b[0m embeddings_cpu \u001b[38;5;241m=\u001b[39m embeddings_tensor\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "Cell \u001b[1;32mIn[15], line 29\u001b[0m, in \u001b[0;36mbio_extract_text_embeddings\u001b[1;34m(text_list, batch_size)\u001b[0m\n\u001b[0;32m     26\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {key: val\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m key, val \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems()}  \u001b[38;5;66;03m# Ensure all tensors are on GPU\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():  \u001b[38;5;66;03m# No gradient tracking for inference\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m biomodel(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Compute mean pooling of token embeddings (Sentence Representation)\u001b[39;00m\n\u001b[0;32m     32\u001b[0m batch_embeddings \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Stays on GPU\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sriha\\anaconda3\\envs\\nest\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\sriha\\anaconda3\\envs\\nest\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\sriha\\anaconda3\\envs\\nest\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1142\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1135\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m   1136\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[0;32m   1137\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[0;32m   1138\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[0;32m   1139\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[0;32m   1140\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m-> 1142\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1153\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1154\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1155\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sriha\\anaconda3\\envs\\nest\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\sriha\\anaconda3\\envs\\nest\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\sriha\\anaconda3\\envs\\nest\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:695\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    684\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    685\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    686\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    692\u001b[0m         output_attentions,\n\u001b[0;32m    693\u001b[0m     )\n\u001b[0;32m    694\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 695\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    703\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    706\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\sriha\\anaconda3\\envs\\nest\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\sriha\\anaconda3\\envs\\nest\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\sriha\\anaconda3\\envs\\nest\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:627\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    624\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    625\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[1;32m--> 627\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[0;32m    629\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    630\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[0;32m    632\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sriha\\anaconda3\\envs\\nest\\lib\\site-packages\\transformers\\pytorch_utils.py:255\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m    252\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[0;32m    253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[1;32m--> 255\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sriha\\anaconda3\\envs\\nest\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:639\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[1;34m(self, attention_output)\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[1;32m--> 639\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    640\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[0;32m    641\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[1;32mc:\\Users\\sriha\\anaconda3\\envs\\nest\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\sriha\\anaconda3\\envs\\nest\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\sriha\\anaconda3\\envs\\nest\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:539\u001b[0m, in \u001b[0;36mBertIntermediate.forward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    538\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m--> 539\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    540\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate_act_fn(hidden_states)\n\u001b[0;32m    541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[1;32mc:\\Users\\sriha\\anaconda3\\envs\\nest\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\sriha\\anaconda3\\envs\\nest\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\sriha\\anaconda3\\envs\\nest\\lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load BioBERT model & tokenizer\n",
    "biotokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\")\n",
    "biomodel = AutoModel.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\").to(\"cuda\")  # Move model to GPU\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Function to extract BioBERT embeddings in GPU-friendly batches\n",
    "def bio_extract_text_embeddings(text_list, batch_size=512):\n",
    "    \"\"\"Extracts embeddings for text data using BioBERT with batch processing on GPU.\"\"\"\n",
    "    embeddings = []\n",
    "    \n",
    "    dataloader = DataLoader(text_list, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    for batch_texts in tqdm(dataloader, desc=\"Processing Batches\", unit=\"batch\"):\n",
    "        # Tokenization and moving inputs to GPU\n",
    "        inputs = biotokenizer(batch_texts, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "        inputs = {key: val.to(device) for key, val in inputs.items()}  # Ensure all tensors are on GPU\n",
    "        \n",
    "        with torch.no_grad():  # No gradient tracking for inference\n",
    "            outputs = biomodel(**inputs)\n",
    "        \n",
    "        # Compute mean pooling of token embeddings (Sentence Representation)\n",
    "        batch_embeddings = outputs.last_hidden_state.mean(dim=1)  # Stays on GPU\n",
    "        \n",
    "        embeddings.append(batch_embeddings)\n",
    "\n",
    "    return torch.cat(embeddings, dim=0)  # Returns a single tensor on GPU\n",
    "\n",
    "# Process 'Unstructured' text attribute in 512-size batches\n",
    "attribute = \"Unstructured\"\n",
    "print(f\"Processing attribute: {attribute}\")\n",
    "\n",
    "bio_text_data = data[attribute].astype(str).tolist()\n",
    "\n",
    "# Extract embeddings using batch processing on GPU\n",
    "embeddings_tensor = bio_extract_text_embeddings(bio_text_data, batch_size=512)  # Returns tensor on GPU\n",
    "\n",
    "# Move embeddings back to CPU for DataFrame creation\n",
    "embeddings_cpu = embeddings_tensor.cpu().numpy()\n",
    "\n",
    "# Create DataFrame with embedding columns\n",
    "bioembedding_df = pd.DataFrame(embeddings_cpu, index=data.index)\n",
    "bioembedding_df.columns = [f\"{attribute}_embed_{i}\" for i in range(embeddings_cpu.shape[1])]\n",
    "\n",
    "# Drop the original attribute column and merge embeddings\n",
    "data = data.drop(columns=[attribute])\n",
    "data = pd.concat([data, bioembedding_df], axis=1)\n",
    "\n",
    "print(\"Embeddings successfully replaced the original attributes.\")\n",
    "data.to_csv('NEST_embed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>NCT Number</th>\n",
       "      <th>Study Status</th>\n",
       "      <th>Conditions</th>\n",
       "      <th>Other Outcome Measures</th>\n",
       "      <th>Sponsor</th>\n",
       "      <th>Collaborators</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Phases</th>\n",
       "      <th>...</th>\n",
       "      <th>Unstructured_embed_758</th>\n",
       "      <th>Unstructured_embed_759</th>\n",
       "      <th>Unstructured_embed_760</th>\n",
       "      <th>Unstructured_embed_761</th>\n",
       "      <th>Unstructured_embed_762</th>\n",
       "      <th>Unstructured_embed_763</th>\n",
       "      <th>Unstructured_embed_764</th>\n",
       "      <th>Unstructured_embed_765</th>\n",
       "      <th>Unstructured_embed_766</th>\n",
       "      <th>Unstructured_embed_767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NCT00559130</td>\n",
       "      <td>Completed</td>\n",
       "      <td>Acute Respiratory Distress Syndrome|Acute Lung...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MedaSorb Technologies, Inc</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ALL</td>\n",
       "      <td>ADULT, OLDER_ADULT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.098257</td>\n",
       "      <td>0.107153</td>\n",
       "      <td>-0.193596</td>\n",
       "      <td>0.135309</td>\n",
       "      <td>0.092266</td>\n",
       "      <td>-0.020007</td>\n",
       "      <td>-0.063864</td>\n",
       "      <td>0.123716</td>\n",
       "      <td>-0.106573</td>\n",
       "      <td>0.031290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>NCT00937664</td>\n",
       "      <td>Not_Completed</td>\n",
       "      <td>Cancer|Solid Tumors|Advanced Solid Malignancies</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AstraZeneca</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ALL</td>\n",
       "      <td>ADULT, OLDER_ADULT</td>\n",
       "      <td>PHASE1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018887</td>\n",
       "      <td>0.115186</td>\n",
       "      <td>-0.080183</td>\n",
       "      <td>0.219760</td>\n",
       "      <td>0.035297</td>\n",
       "      <td>0.034310</td>\n",
       "      <td>0.078921</td>\n",
       "      <td>-0.039460</td>\n",
       "      <td>-0.072698</td>\n",
       "      <td>-0.054465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>NCT00441597</td>\n",
       "      <td>Completed</td>\n",
       "      <td>Ischemia Reperfusion Injury|Cardiovascular Dis...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Radboud University Medical Center</td>\n",
       "      <td>Pfizer</td>\n",
       "      <td>MALE</td>\n",
       "      <td>ADULT</td>\n",
       "      <td>PHASE4</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.112456</td>\n",
       "      <td>0.116638</td>\n",
       "      <td>-0.116839</td>\n",
       "      <td>0.092405</td>\n",
       "      <td>-0.204348</td>\n",
       "      <td>0.031370</td>\n",
       "      <td>-0.016791</td>\n",
       "      <td>0.158138</td>\n",
       "      <td>-0.017895</td>\n",
       "      <td>0.021559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>NCT03296228</td>\n",
       "      <td>Completed</td>\n",
       "      <td>Adolescent Idiopathic Scoliosis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The University of Hong Kong</td>\n",
       "      <td>AO Foundation, AO Spine</td>\n",
       "      <td>ALL</td>\n",
       "      <td>CHILD, ADULT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.013137</td>\n",
       "      <td>-0.063034</td>\n",
       "      <td>0.019267</td>\n",
       "      <td>-0.061944</td>\n",
       "      <td>-0.091516</td>\n",
       "      <td>-0.104973</td>\n",
       "      <td>0.211876</td>\n",
       "      <td>0.175831</td>\n",
       "      <td>-0.115984</td>\n",
       "      <td>-0.035802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>NCT00421603</td>\n",
       "      <td>Completed</td>\n",
       "      <td>Cocaine Dependence</td>\n",
       "      <td>NaN</td>\n",
       "      <td>New York State Psychiatric Institute</td>\n",
       "      <td>National Institute on Drug Abuse (NIDA)</td>\n",
       "      <td>ALL</td>\n",
       "      <td>ADULT</td>\n",
       "      <td>PHASE2</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024513</td>\n",
       "      <td>0.121777</td>\n",
       "      <td>0.032171</td>\n",
       "      <td>0.037329</td>\n",
       "      <td>-0.005663</td>\n",
       "      <td>-0.278086</td>\n",
       "      <td>0.064402</td>\n",
       "      <td>0.270871</td>\n",
       "      <td>-0.001445</td>\n",
       "      <td>0.048874</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 786 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0   NCT Number   Study Status  \\\n",
       "0           0  NCT00559130      Completed   \n",
       "1           1  NCT00937664  Not_Completed   \n",
       "2           2  NCT00441597      Completed   \n",
       "3           3  NCT03296228      Completed   \n",
       "4           4  NCT00421603      Completed   \n",
       "\n",
       "                                          Conditions Other Outcome Measures  \\\n",
       "0  Acute Respiratory Distress Syndrome|Acute Lung...                    NaN   \n",
       "1    Cancer|Solid Tumors|Advanced Solid Malignancies                    NaN   \n",
       "2  Ischemia Reperfusion Injury|Cardiovascular Dis...                    NaN   \n",
       "3                    Adolescent Idiopathic Scoliosis                    NaN   \n",
       "4                                 Cocaine Dependence                    NaN   \n",
       "\n",
       "                                Sponsor  \\\n",
       "0            MedaSorb Technologies, Inc   \n",
       "1                           AstraZeneca   \n",
       "2     Radboud University Medical Center   \n",
       "3           The University of Hong Kong   \n",
       "4  New York State Psychiatric Institute   \n",
       "\n",
       "                             Collaborators   Sex                 Age  Phases  \\\n",
       "0                                      NaN   ALL  ADULT, OLDER_ADULT     NaN   \n",
       "1                                      NaN   ALL  ADULT, OLDER_ADULT  PHASE1   \n",
       "2                                   Pfizer  MALE               ADULT  PHASE4   \n",
       "3                  AO Foundation, AO Spine   ALL        CHILD, ADULT     NaN   \n",
       "4  National Institute on Drug Abuse (NIDA)   ALL               ADULT  PHASE2   \n",
       "\n",
       "   ...  Unstructured_embed_758 Unstructured_embed_759 Unstructured_embed_760  \\\n",
       "0  ...               -0.098257               0.107153              -0.193596   \n",
       "1  ...                0.018887               0.115186              -0.080183   \n",
       "2  ...               -0.112456               0.116638              -0.116839   \n",
       "3  ...               -0.013137              -0.063034               0.019267   \n",
       "4  ...               -0.024513               0.121777               0.032171   \n",
       "\n",
       "  Unstructured_embed_761  Unstructured_embed_762  Unstructured_embed_763  \\\n",
       "0               0.135309                0.092266               -0.020007   \n",
       "1               0.219760                0.035297                0.034310   \n",
       "2               0.092405               -0.204348                0.031370   \n",
       "3              -0.061944               -0.091516               -0.104973   \n",
       "4               0.037329               -0.005663               -0.278086   \n",
       "\n",
       "  Unstructured_embed_764 Unstructured_embed_765  Unstructured_embed_766  \\\n",
       "0              -0.063864               0.123716               -0.106573   \n",
       "1               0.078921              -0.039460               -0.072698   \n",
       "2              -0.016791               0.158138               -0.017895   \n",
       "3               0.211876               0.175831               -0.115984   \n",
       "4               0.064402               0.270871               -0.001445   \n",
       "\n",
       "   Unstructured_embed_767  \n",
       "0                0.031290  \n",
       "1               -0.054465  \n",
       "2                0.021559  \n",
       "3               -0.035802  \n",
       "4                0.048874  \n",
       "\n",
       "[5 rows x 786 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.to_csv('NEST_embed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "354f994edc044e94a90acf5155958fa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/385 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sriha\\anaconda3\\envs\\nest\\lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\sriha\\.cache\\huggingface\\hub\\models--emilyalsentzer--Bio_ClinicalBERT. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a108d71619341c581bce83a56a0e271",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d321236b92c4b71beb7ba3d893c1521",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "582deafe22144adf8b9f94591ad2729e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clinicaltokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "clinicalmodel = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clinical_extract_text_embeddings(text_data):\n",
    "\n",
    "    embeddings = []\n",
    "    for text in text_data:\n",
    "        inputs = clinicaltokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "        outputs = clinicalmodel(**inputs)\n",
    "        embeddings.append(outputs.last_hidden_state.mean(dim=1).detach().numpy())\n",
    "\n",
    "    return np.vstack(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>NCT Number</th>\n",
       "      <th>Study Title</th>\n",
       "      <th>Study Status</th>\n",
       "      <th>Brief Summary</th>\n",
       "      <th>Conditions</th>\n",
       "      <th>Primary Outcome Measures</th>\n",
       "      <th>Secondary Outcome Measures</th>\n",
       "      <th>Other Outcome Measures</th>\n",
       "      <th>Sponsor</th>\n",
       "      <th>...</th>\n",
       "      <th>Age</th>\n",
       "      <th>Phases</th>\n",
       "      <th>Enrollment</th>\n",
       "      <th>Funder Type</th>\n",
       "      <th>Study Type</th>\n",
       "      <th>Study Design</th>\n",
       "      <th>Start Month</th>\n",
       "      <th>Start Quarter</th>\n",
       "      <th>Condition Category</th>\n",
       "      <th>Conditions_Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NCT00559130</td>\n",
       "      <td>Efficacy Study of CytoSorb Hemoperfusion Devic...</td>\n",
       "      <td>Completed</td>\n",
       "      <td>The hypothesis of this study is use of CytoSor...</td>\n",
       "      <td>Acute Respiratory Distress Syndrome|Acute Lung...</td>\n",
       "      <td>Relative IL-6 levels as a percent (%) of basel...</td>\n",
       "      <td>Ventilator Free Days, Reduction cytokines TNF-...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MedaSorb Technologies, Inc</td>\n",
       "      <td>...</td>\n",
       "      <td>ADULT, OLDER_ADULT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>INDUSTRY</td>\n",
       "      <td>INTERVENTIONAL</td>\n",
       "      <td>Allocation: RANDOMIZED|Intervention Model: PAR...</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>Other Rare or Unclassified</td>\n",
       "      <td>Other Rare or Unclassified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>NCT00937664</td>\n",
       "      <td>Safety and Tolerability Study of AZD7762 in Co...</td>\n",
       "      <td>Not_Completed</td>\n",
       "      <td>The primary purpose of this study is to find o...</td>\n",
       "      <td>Cancer|Solid Tumors|Advanced Solid Malignancies</td>\n",
       "      <td>Assessment of adverse events (based on CTCAE v...</td>\n",
       "      <td>Pharmacokinetic effect of AZD7762 when adminis...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AstraZeneca</td>\n",
       "      <td>...</td>\n",
       "      <td>ADULT, OLDER_ADULT</td>\n",
       "      <td>PHASE1</td>\n",
       "      <td>24.0</td>\n",
       "      <td>INDUSTRY</td>\n",
       "      <td>INTERVENTIONAL</td>\n",
       "      <td>Allocation: NON_RANDOMIZED|Intervention Model:...</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>Oncology</td>\n",
       "      <td>Oncology</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0   NCT Number                                        Study Title  \\\n",
       "0           0  NCT00559130  Efficacy Study of CytoSorb Hemoperfusion Devic...   \n",
       "1           1  NCT00937664  Safety and Tolerability Study of AZD7762 in Co...   \n",
       "\n",
       "    Study Status                                      Brief Summary  \\\n",
       "0      Completed  The hypothesis of this study is use of CytoSor...   \n",
       "1  Not_Completed  The primary purpose of this study is to find o...   \n",
       "\n",
       "                                          Conditions  \\\n",
       "0  Acute Respiratory Distress Syndrome|Acute Lung...   \n",
       "1    Cancer|Solid Tumors|Advanced Solid Malignancies   \n",
       "\n",
       "                            Primary Outcome Measures  \\\n",
       "0  Relative IL-6 levels as a percent (%) of basel...   \n",
       "1  Assessment of adverse events (based on CTCAE v...   \n",
       "\n",
       "                          Secondary Outcome Measures Other Outcome Measures  \\\n",
       "0  Ventilator Free Days, Reduction cytokines TNF-...                    NaN   \n",
       "1  Pharmacokinetic effect of AZD7762 when adminis...                    NaN   \n",
       "\n",
       "                      Sponsor  ...                 Age  Phases Enrollment  \\\n",
       "0  MedaSorb Technologies, Inc  ...  ADULT, OLDER_ADULT     NaN      100.0   \n",
       "1                 AstraZeneca  ...  ADULT, OLDER_ADULT  PHASE1       24.0   \n",
       "\n",
       "  Funder Type      Study Type  \\\n",
       "0    INDUSTRY  INTERVENTIONAL   \n",
       "1    INDUSTRY  INTERVENTIONAL   \n",
       "\n",
       "                                        Study Design Start Month  \\\n",
       "0  Allocation: RANDOMIZED|Intervention Model: PAR...          11   \n",
       "1  Allocation: NON_RANDOMIZED|Intervention Model:...           7   \n",
       "\n",
       "  Start Quarter          Condition Category         Conditions_Category  \n",
       "0             4  Other Rare or Unclassified  Other Rare or Unclassified  \n",
       "1             3                    Oncology                    Oncology  \n",
       "\n",
       "[2 rows x 22 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.head(2)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Attributes:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing attribute: Primary Outcome Measures\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Attributes:   0%|          | 0/2 [05:39<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m clinical_text_data \u001b[38;5;241m=\u001b[39m data[attribute]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Extract embeddings and ensure proper NumPy array structure\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mclinical_extract_text_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbio_text_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Convert to torch tensor and move to device (ensure dtype compatibility)\u001b[39;00m\n\u001b[0;32m     11\u001b[0m clinical_text_data_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(embeddings, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "Cell \u001b[1;32mIn[27], line 6\u001b[0m, in \u001b[0;36mclinical_extract_text_embeddings\u001b[1;34m(text_data)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m text_data:\n\u001b[0;32m      5\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m clinicaltokenizer(text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m clinicalmodel(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m      7\u001b[0m     embeddings\u001b[38;5;241m.\u001b[39mappend(outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mvstack(embeddings)\n",
      "File \u001b[1;32mc:\\Users\\sriha\\anaconda3\\envs\\nest\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\sriha\\anaconda3\\envs\\nest\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\sriha\\anaconda3\\envs\\nest\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1142\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1135\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m   1136\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[0;32m   1137\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[0;32m   1138\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[0;32m   1139\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[0;32m   1140\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m-> 1142\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1153\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1154\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1155\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sriha\\anaconda3\\envs\\nest\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\sriha\\anaconda3\\envs\\nest\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\sriha\\anaconda3\\envs\\nest\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:695\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    684\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    685\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    686\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    692\u001b[0m         output_attentions,\n\u001b[0;32m    693\u001b[0m     )\n\u001b[0;32m    694\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 695\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    703\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    706\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\sriha\\anaconda3\\envs\\nest\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\sriha\\anaconda3\\envs\\nest\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\sriha\\anaconda3\\envs\\nest\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:627\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    624\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    625\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[1;32m--> 627\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[0;32m    629\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    630\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[0;32m    632\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sriha\\anaconda3\\envs\\nest\\lib\\site-packages\\transformers\\pytorch_utils.py:255\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m    252\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[0;32m    253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[1;32m--> 255\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sriha\\anaconda3\\envs\\nest\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:640\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[1;34m(self, attention_output)\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[0;32m    639\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate(attention_output)\n\u001b[1;32m--> 640\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintermediate_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    641\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[1;32mc:\\Users\\sriha\\anaconda3\\envs\\nest\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\sriha\\anaconda3\\envs\\nest\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\sriha\\anaconda3\\envs\\nest\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:552\u001b[0m, in \u001b[0;36mBertOutput.forward\u001b[1;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[0;32m    551\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor, input_tensor: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m--> 552\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    553\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[0;32m    554\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_states \u001b[38;5;241m+\u001b[39m input_tensor)\n",
      "File \u001b[1;32mc:\\Users\\sriha\\anaconda3\\envs\\nest\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\sriha\\anaconda3\\envs\\nest\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\sriha\\anaconda3\\envs\\nest\\lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for attribute in tqdm(['Primary Outcome Measures', 'Secondary Outcome Measures'], desc=\"Processing Attributes\"):\n",
    "    print(f\"Processing attribute: {attribute}\")\n",
    "    \n",
    "    # Convert text column to string\n",
    "    clinical_text_data = data[attribute].astype(str).tolist()\n",
    "\n",
    "    # Extract embeddings and ensure proper NumPy array structure\n",
    "    embeddings = clinical_extract_text_embeddings(bio_text_data)\n",
    "    \n",
    "    # Convert to torch tensor and move to device (ensure dtype compatibility)\n",
    "    clinical_text_data_tensor = torch.tensor(embeddings, dtype=torch.float32).to(device)\n",
    "\n",
    "    # Move embeddings back to CPU for DataFrame creation\n",
    "    clinicalembeddings_cpu = clinical_text_data_tensor.cpu().numpy()\n",
    "\n",
    "    # Create DataFrame with embedding columns\n",
    "    clinicalembedding_df = pd.DataFrame(clinicalembeddings_cpu, index=data.index)\n",
    "    clinicalembedding_df.columns = [f\"{attribute}_embed_{i}\" for i in range(embeddings_cpu.shape[1])]\n",
    "\n",
    "    # Drop the original attribute column and merge embeddings\n",
    "    data = data.drop(columns=[attribute])\n",
    "    data = pd.concat([data, clinicalembedding_df], axis=1)\n",
    "\n",
    "print(\"Embeddings successfully replaced the original attributes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clinical_embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Step 4: Combine Features and Embeddings\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Combine structured features with embeddings\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m X_combined \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mhstack((numerical_scaled, categorical_encoded, bio_embeddings, \u001b[43mclinical_embeddings\u001b[49m))\n\u001b[0;32m      4\u001b[0m y \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStudy Status\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# Target column\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Train-Test Split for Combined Data\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'clinical_embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "# Step 4: Combine Features and Embeddings\n",
    "# Combine structured features with embeddings\n",
    "X_combined = np.hstack((numerical_scaled, categorical_encoded, bio_embeddings, clinical_embeddings))\n",
    "y = data['Study Status']  # Target column\n",
    "\n",
    "# Train-Test Split for Combined Data\n",
    "X_train_combined, X_test_combined, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: TabTransformer for Combined Data\n",
    "def train_tab_transformer(X_train, y_train):\n",
    "    \"\"\"Trains a TabTransformer model on the combined structured and text data.\"\"\"\n",
    "    model = TabTransformerModel(\n",
    "        task=\"classification\",\n",
    "        input_dim=X_train.shape[1],\n",
    "        n_classes=len(np.unique(y_train)),\n",
    "        cat_idxs=[],\n",
    "        cat_dims=[],\n",
    "        n_dnn_layers=2,\n",
    "        n_dnn_units=64,\n",
    "        learning_rate=1e-3,\n",
    "        batch_size=32,\n",
    "        epochs=10\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "# Train TabTransformer\n",
    "tab_transformer_model = train_tab_transformer(X_train_combined, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Evaluate TabTransformer\n",
    "predictions = tab_transformer_model.predict(X_test_combined)\n",
    "print(\"TabTransformer Classification Report:\\n\", classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
